{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coordinate-clothing",
   "metadata": {
    "id": "hawaiian-reporter"
   },
   "source": [
    "# Ways to Speed up RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unnecessary-diagnosis",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uc7akLCa4zrz",
    "outputId": "6589ebc8-73db-486b-a463-604cd5feeed2"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
    "\n",
    "pip install ptan>=0.7 pytorch-ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-stability",
   "metadata": {
    "id": "hollow-preparation"
   },
   "source": [
    "## Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatal-aaron",
   "metadata": {
    "id": "headed-combination"
   },
   "outputs": [],
   "source": [
    "# flake8: noqa: E402,I001\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from types import SimpleNamespace\n",
    "from typing import Any, Collection, Dict, Iterable, List, NamedTuple, Union\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ptan\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ptan.actions import EpsilonGreedyActionSelector\n",
    "from ptan.agent import DQNAgent, TargetNet\n",
    "from ptan.experience import (\n",
    "    ExperienceFirstLast,\n",
    "    ExperienceReplayBuffer,\n",
    "    ExperienceSource,\n",
    "    ExperienceSourceFirstLast,\n",
    "    ExperienceSourceRollouts,\n",
    ")\n",
    "from ptan.ignite import (\n",
    "    EndOfEpisodeHandler,\n",
    "    EpisodeEvents,\n",
    "    EpisodeFPSHandler,\n",
    "    PeriodEvents,\n",
    "    PeriodicEvents,\n",
    ")\n",
    "\n",
    "ExpSource = Union[\n",
    "    ExperienceSource,\n",
    "    ExperienceSourceFirstLast,\n",
    "    ExperienceSourceRollouts,\n",
    "]\n",
    "\n",
    "\n",
    "# RNG seed\n",
    "SEED = 123\n",
    "\n",
    "\n",
    "# Hyperparameters for different Atari games\n",
    "HYPERPARAMS = {\n",
    "    \"pong\": SimpleNamespace(\n",
    "        **{\n",
    "            \"env_name\": \"PongNoFrameskip-v4\",\n",
    "            \"stop_reward\": 18.0,\n",
    "            \"run_name\": \"pong\",\n",
    "            \"log_period\": 10,\n",
    "            \"replay_size\": 100000,\n",
    "            \"replay_initial\": 10000,\n",
    "            \"target_net_sync\": 1000,\n",
    "            \"epsilon_frames\": 10 ** 5,\n",
    "            \"epsilon_start\": 1.0,\n",
    "            \"epsilon_final\": 0.02,\n",
    "            \"learning_rate\": 0.0001,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 32,\n",
    "        }\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-organic",
   "metadata": {
    "id": "seasonal-learning"
   },
   "source": [
    "## Experience Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tested-saver",
   "metadata": {
    "id": "brazilian-mexico"
   },
   "outputs": [],
   "source": [
    "def batch_stream(\n",
    "    buffer: ExperienceReplayBuffer,\n",
    "    initial: int,\n",
    "    batch_size: int,\n",
    ") -> Iterable[List[ExperienceFirstLast]]:\n",
    "    \"\"\"\n",
    "    Fills up the buffer with `initial` capacity and then\n",
    "    produces an infinite stream of sampled batches from it\n",
    "    while adding new experiences.\n",
    "    \"\"\"\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)\n",
    "\n",
    "\n",
    "class ExperienceBatch(NamedTuple):\n",
    "    states: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    last_states: np.ndarray\n",
    "    dones: np.ndarray\n",
    "\n",
    "\n",
    "def unpack_batch(batch: Collection[ExperienceFirstLast]) -> ExperienceBatch:\n",
    "    states, actions, rewards, last_states, dones = [], [], [], [], []\n",
    "\n",
    "    # Unzip batch experiences into components\n",
    "    #  - When epoch ends last_state is the initial state\n",
    "    #  - This is ok because the result will be masked anyway\n",
    "    for exp in batch:\n",
    "\n",
    "        done = exp.last_state is None\n",
    "        state = np.array(exp.state)\n",
    "        last_state = state if done else np.array(exp.last_state)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        last_states.append(last_state)\n",
    "        dones.append(done)\n",
    "\n",
    "    # Wrap all components into numpy and a named tuple\n",
    "    return ExperienceBatch(\n",
    "        states=np.array(states, copy=False),\n",
    "        actions=np.array(actions),\n",
    "        rewards=np.array(rewards, dtype=np.float32),\n",
    "        last_states=np.array(last_states, copy=False),\n",
    "        dones=np.array(dones, dtype=np.uint8),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-charge",
   "metadata": {
    "id": "desperate-ontario"
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "differential-information",
   "metadata": {
    "id": "dimensional-database"
   },
   "outputs": [],
   "source": [
    "def dqn_loss(\n",
    "    batch: Collection[ExperienceFirstLast],\n",
    "    net: nn.Module,\n",
    "    target_net: TargetNet,\n",
    "    gamma: float,\n",
    "    device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    # Unwrap experience batch into components\n",
    "    exp_batch = unpack_batch(batch)\n",
    "\n",
    "    # Turn them into tensors\n",
    "    states = torch.tensor(exp_batch.states).to(device)\n",
    "    next_states = torch.tensor(exp_batch.last_states).to(device)\n",
    "    actions = torch.tensor(exp_batch.actions).to(device)\n",
    "    rewards = torch.tensor(exp_batch.rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(exp_batch.dones).to(device)\n",
    "\n",
    "    # Compute Q values for all actions played in batch states\n",
    "    q_values = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Do not add these operations to the computation graph for autograd\n",
    "    with torch.no_grad():\n",
    "        # Compute fixed values of future states using the target DQN\n",
    "        #  - And zero out terminal states\n",
    "        future_values = target_net(next_states).max(1)[0]\n",
    "        future_values[done_mask] = 0.0\n",
    "\n",
    "    # Compute TD targets and their MSE to current Q values\n",
    "    td_targets = rewards + gamma * future_values.detach()\n",
    "    return nn.MSELoss()(q_values, td_targets)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_state_values(\n",
    "    states: np.ndarray,\n",
    "    net: nn.Module,\n",
    "    device: str = \"cpu\",\n",
    ") -> np.ndarray:\n",
    "    mean_values = []\n",
    "\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states = torch.tensor(batch).to(device)\n",
    "        q_values = net(states)\n",
    "        best_q_values = q_values.max(1)[0]\n",
    "        mean_values.append(best_q_values.mean().item())\n",
    "\n",
    "    return np.mean(mean_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-understanding",
   "metadata": {
    "id": "noticed-exclusion"
   },
   "source": [
    "## Epsilon Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "civilian-edition",
   "metadata": {
    "id": "scientific-locator"
   },
   "outputs": [],
   "source": [
    "class EpsilonScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        selector: EpsilonGreedyActionSelector,\n",
    "        params: SimpleNamespace,\n",
    "    ) -> None:\n",
    "        self.selector = selector\n",
    "        self.params = params\n",
    "        self.set_epsilon(t=0)\n",
    "\n",
    "    def set_epsilon(self, t: int) -> None:\n",
    "        \"\"\"\n",
    "        Set epsilon for current iteration t in the associated selector.\n",
    "        \"\"\"\n",
    "        eps = self.params.epsilon_start - t / self.params.epsilon_frames\n",
    "        self.selector.epsilon = max(self.params.epsilon_final, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-booking",
   "metadata": {
    "id": "young-attachment"
   },
   "source": [
    "## Ignite Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "considered-favor",
   "metadata": {
    "id": "eligible-plumbing"
   },
   "outputs": [],
   "source": [
    "def setup_ignite(\n",
    "    engine: Engine,\n",
    "    params: SimpleNamespace,\n",
    "    exp_source: ExpSource,\n",
    "    run_name: str,\n",
    "    extra_metrics: Iterable[str] = (),\n",
    "):\n",
    "    # Get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # Register some PTAN handlers to the Ingite engine\n",
    "    handler = EndOfEpisodeHandler(\n",
    "        exp_source, bound_avg_reward=params.stop_reward\n",
    "    )\n",
    "    handler.attach(engine)\n",
    "    EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    # Log training progress at the end of each episode\n",
    "    @engine.on(EpisodeEvents.EPISODE_COMPLETED(every=params.log_period))\n",
    "    def episode_completed(trainer: Engine) -> None:\n",
    "        passed = trainer.state.metrics.get(\"time_passed\", 0)\n",
    "        elapsed = timedelta(seconds=int(passed))\n",
    "        avg_fps = trainer.state.metrics.get(\"avg_fps\", 0)\n",
    "        print(\n",
    "            f\"Episode {trainer.state.episode}: \"\n",
    "            f\"reward={trainer.state.episode_reward:.0f}, \"\n",
    "            f\"steps={trainer.state.episode_steps}, \"\n",
    "            f\"speed={avg_fps:.1f} fps, \"\n",
    "            f\"elapsed={elapsed}\"\n",
    "        )\n",
    "\n",
    "    # Log and terminate when good enough solution has been found\n",
    "    @engine.on(EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine) -> None:\n",
    "        passed = trainer.state.metrics[\"time_passed\"]\n",
    "        elapsed = timedelta(seconds=int(passed))\n",
    "        print(\n",
    "            f\"Game solved in {elapsed}, \"\n",
    "            f\"after {trainer.state.episode} episodes \"\n",
    "            f\"and {trainer.state.iteration} iterations!\"\n",
    "        )\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "    # Setup TensorBoard logger with fresh logging directory\n",
    "    now = datetime.now().isoformat(timespec=\"minutes\").replace(\":\", \"\")\n",
    "    logdir = f\"runs/{now}-{params.run_name}-{run_name}\"\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "\n",
    "    # Register smoothened loss as a metric\n",
    "    run_avg = RunningAverage(output_transform=lambda v: v[\"loss\"])\n",
    "    run_avg.attach(engine, \"avg_loss\")\n",
    "\n",
    "    # Record following metrics at the end of each epoch\n",
    "    metrics = [\"reward\", \"steps\", \"avg_reward\"]\n",
    "    tb.attach(\n",
    "        engine=engine,\n",
    "        log_handler=tb_logger.OutputHandler(\n",
    "            tag=\"episodes\", metric_names=metrics\n",
    "        ),\n",
    "        event_name=EpisodeEvents.EPISODE_COMPLETED,\n",
    "    )\n",
    "\n",
    "    # Write other metrics to TensorBoard every 100 iterations\n",
    "    PeriodicEvents().attach(engine)\n",
    "\n",
    "    handler = tb_logger.OutputHandler(\n",
    "        tag=\"train\",\n",
    "        metric_names=[\"avg_loss\", \"avg_fps\"] + list(extra_metrics),\n",
    "        output_transform=lambda a: a,\n",
    "    )\n",
    "\n",
    "    tb.attach(\n",
    "        engine=engine,\n",
    "        log_handler=handler,\n",
    "        event_name=PeriodEvents.ITERS_100_COMPLETED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-expansion",
   "metadata": {
    "id": "0_0s-MgM4ECO"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "documentary-trail",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4obVKZ34MND",
    "outputId": "28562526-0a82-47ad-d5d2-7b9ced479c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu to run computations.\n",
      "The Atari environment is 'PongNoFrameskip-v4'.\n"
     ]
    }
   ],
   "source": [
    "# Determine where computations will take place\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} to run computations.\")\n",
    "\n",
    "# Get the set of hyperparameters for Atari Pong\n",
    "params = HYPERPARAMS[\"pong\"]\n",
    "print(f\"The Atari environment is '{params.env_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-valentine",
   "metadata": {
    "id": "multiple-french"
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "electric-governor",
   "metadata": {
    "id": "attempted-indonesia"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple  # noqa\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape: Tuple[int, ...], n_actions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        n_conv_inputs = input_shape[0]\n",
    "\n",
    "        # Stack of 2D convolutional layers with ReLU activations\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(n_conv_inputs, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        n_fc_inputs = self._conv_output_dim(input_shape)\n",
    "\n",
    "        # Fully connected layers for the regression part\n",
    "        #  - Outputs Q(., a) for each action a\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_fc_inputs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def _conv_output_dim(self, shape: Tuple[int, ...]) -> int:\n",
    "        dummy_conv_input = torch.zeros(1, *shape, dtype=torch.float32)\n",
    "        dummy_conv_output = self.conv(dummy_conv_input)\n",
    "        return int(np.prod(dummy_conv_output.size()))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Scale the inputs\n",
    "        inputs = x.float() / 256\n",
    "        # `view(batch_size, -1)` flattens all the feature dimensions\n",
    "        conv_out = self.conv(inputs).view(inputs.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-server",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-regular",
   "metadata": {
    "id": "aKEWo1vCAcJf"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-sunset",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "08_dqn_extensions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
