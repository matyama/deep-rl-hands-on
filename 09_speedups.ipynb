{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "08_dqn_extensions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coordinate-clothing"
      },
      "source": [
        "# Ways to Speed up RL"
      ],
      "id": "coordinate-clothing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unnecessary-diagnosis",
        "outputId": "0bab32b8-5a33-4cb8-e158-4418a005dd36"
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
        "\n",
        "pip install ptan>=0.7 pytorch-ignite"
      ],
      "id": "unnecessary-diagnosis",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on Google Colab, therefore installing dependencies...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appreciated-stability"
      },
      "source": [
        "## Imports and Hyperparameters"
      ],
      "id": "appreciated-stability"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatal-aaron",
        "outputId": "93e85425-46c9-4afc-dd2e-f80864f95839",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# flake8: noqa: E402,I001\n",
        "\n",
        "import random\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from types import SimpleNamespace\n",
        "from typing import Collection, Iterable, List, NamedTuple, Tuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import ptan\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn as nn\n",
        "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
        "from ignite.engine import Engine\n",
        "from ignite.metrics import RunningAverage\n",
        "from ptan.actions import EpsilonGreedyActionSelector\n",
        "from ptan.agent import DQNAgent, TargetNet\n",
        "from ptan.experience import (\n",
        "    ExperienceFirstLast,\n",
        "    ExperienceReplayBuffer,\n",
        "    ExperienceSourceFirstLast,\n",
        ")\n",
        "from ptan.ignite import (\n",
        "    EndOfEpisodeHandler,\n",
        "    EpisodeEvents,\n",
        "    EpisodeFPSHandler,\n",
        "    PeriodEvents,\n",
        "    PeriodicEvents,\n",
        ")\n",
        "\n",
        "# Get rid of missing metrics warning\n",
        "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Determine where computations will take place\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {DEVICE} to run computations.\")\n",
        "\n",
        "# RNG seed\n",
        "SEED = 123\n",
        "\n",
        "# Hyperparameters\n",
        "PARAMS = SimpleNamespace(\n",
        "    **{\n",
        "        \"env_name\": \"PongNoFrameskip-v4\",\n",
        "        \"stop_reward\": 18.0,\n",
        "        \"run_name\": \"pong\",\n",
        "        \"log_period\": 10,\n",
        "        \"replay_size\": 100000,\n",
        "        \"replay_initial\": 10000,\n",
        "        \"target_net_sync\": 1000,\n",
        "        \"epsilon_frames\": 10 ** 5,\n",
        "        \"epsilon_start\": 1.0,\n",
        "        \"epsilon_final\": 0.02,\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"gamma\": 0.99,\n",
        "        \"batch_size\": 32,\n",
        "        \"batch_mul\": 4,\n",
        "    }\n",
        ")"
      ],
      "id": "fatal-aaron",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda to run computations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "individual-valentine"
      },
      "source": [
        "## DQN & Loss Function\n",
        "To show performance optimization techniques it will be sufficient to use the baseline DQN from previous chapter."
      ],
      "id": "individual-valentine"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "electric-governor"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape: Tuple[int, ...], n_actions: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        n_conv_inputs = input_shape[0]\n",
        "\n",
        "        # 2D convolutional layers\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(n_conv_inputs, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        n_fc_inputs = self._conv_output_dim(input_shape)\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_fc_inputs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions),\n",
        "        )\n",
        "\n",
        "    def _conv_output_dim(self, shape: Tuple[int, ...]) -> int:  \n",
        "        dummy_conv_input = torch.zeros(1, *shape, dtype=torch.float32)\n",
        "        dummy_conv_output = self.conv(torch.autograd.Variable(dummy_conv_input))\n",
        "        return int(np.prod(dummy_conv_output.size()))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        inputs = x.float() / 256\n",
        "        conv_out = self.conv(inputs).view(inputs.size()[0], -1)\n",
        "        return self.fc(conv_out)\n",
        "\n",
        "\n",
        "def unpack(batch: Collection[ExperienceFirstLast]) -> Tuple[np.ndarray, ...]:\n",
        "    states, actions, rewards, last_states, dones = [], [], [], [], []\n",
        "\n",
        "    # Unzip batch experiences into components\n",
        "    #  - When epoch ends last_state is the initial state\n",
        "    #  - This is ok because the result will be masked anyway\n",
        "    for exp in batch:\n",
        "\n",
        "        done = exp.last_state is None\n",
        "        state = np.array(exp.state)\n",
        "        last_state = state if done else np.array(exp.last_state)\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(exp.action)\n",
        "        rewards.append(exp.reward)\n",
        "        last_states.append(last_state)\n",
        "        dones.append(done)\n",
        "\n",
        "    # Wrap all components into numpy and return a tuple\n",
        "    return (\n",
        "        np.array(states, copy=False),\n",
        "        np.array(actions),\n",
        "        np.array(rewards, dtype=np.float32),\n",
        "        np.array(last_states, copy=False),\n",
        "        np.array(dones, dtype=np.uint8),\n",
        "    )\n",
        "\n",
        "\n",
        "def dqn_loss(\n",
        "    batch: Collection[ExperienceFirstLast],\n",
        "    net: nn.Module,\n",
        "    target_net: TargetNet,\n",
        "    gamma: float,\n",
        "    device: str = \"cpu\",\n",
        ") -> torch.Tensor:\n",
        "    # Unwrap experience batch into components\n",
        "    states, actions, rewards, next_states, dones = unpack(batch)\n",
        "\n",
        "    # Turn them into tensors\n",
        "    states = torch.tensor(states).to(device)\n",
        "    next_states = torch.tensor(next_states).to(device)\n",
        "    actions = torch.tensor(actions).to(device)\n",
        "    rewards = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(dones).to(device)\n",
        "\n",
        "    # Compute Q values for all actions played in batch states\n",
        "    q_values = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # Do not add these operations to the computation graph for autograd\n",
        "    with torch.no_grad():\n",
        "        # Compute fixed values of future states using the target DQN\n",
        "        #  - And zero out terminal states\n",
        "        future_values = target_net(next_states).max(1)[0]\n",
        "        future_values[done_mask] = 0.0\n",
        "\n",
        "    # Compute TD targets and their MSE to current Q values\n",
        "    td_targets = rewards + gamma * future_values.detach()\n",
        "    return nn.MSELoss()(q_values, td_targets)"
      ],
      "id": "electric-governor",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "short-understanding"
      },
      "source": [
        "## Epsilon Schedule"
      ],
      "id": "short-understanding"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civilian-edition"
      },
      "source": [
        "class EpsilonScheduler:\n",
        "    def __init__(\n",
        "        self,\n",
        "        selector: EpsilonGreedyActionSelector,\n",
        "        params: SimpleNamespace,\n",
        "    ) -> None:\n",
        "        self.selector = selector\n",
        "        self.epsilon_start = params.epsilon_start\n",
        "        self.epsilon_final = params.epsilon_final\n",
        "        self.epsilon_frames = params.epsilon_frames\n",
        "        self.set_epsilon(step=0)\n",
        "\n",
        "    def set_epsilon(self, step: int) -> None:\n",
        "        \"\"\"\n",
        "        Set epsilon for current step in the associated selector.\n",
        "        \"\"\"\n",
        "        eps = self.epsilon_start - step / self.epsilon_frames\n",
        "        self.selector.epsilon = max(self.epsilon_final, eps)"
      ],
      "id": "civilian-edition",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guilty-server"
      },
      "source": [
        "## Play Process\n",
        "\n"
      ],
      "id": "guilty-server"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFM2XcHe8ItU"
      },
      "source": [
        "class EpisodeEnded(NamedTuple):\n",
        "    reward: float\n",
        "    steps: int\n",
        "    epsilon: float\n",
        "\n",
        "\n",
        "def play(\n",
        "    net: DQN,\n",
        "    exp_queue: mp.Queue,\n",
        "    params: SimpleNamespace,\n",
        "    device: str = DEVICE,\n",
        "    seed: int = SEED,\n",
        ") -> None:\n",
        "    \n",
        "    # TODO: make_atari and wrap_deepmind\n",
        "    # Setup new environment\n",
        "    env = wrap_deepmind(\n",
        "        env=make_atari(params.env_name, skip_noop=True, skip_maxskip=True),\n",
        "        pytorch_img=True,\n",
        "        frame_stack=True,\n",
        "        frame_stack_count=2,\n",
        "    )\n",
        "    env.seed(seed)\n",
        "\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # Initialize the DQN agent and experience source as usual\n",
        "\n",
        "    selector = EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
        "    epsilon_scheduler = EpsilonScheduler(selector, params)\n",
        "    agent = DQNAgent(net, selector, device=device)\n",
        "    exp_source = ExperienceSourceFirstLast(env, agent, gamma=params.gamma)\n",
        "\n",
        "    # Retrieve experiences from the environment\n",
        "    for frame, exp in enumerate(exp_source):\n",
        "\n",
        "        # Decay epsilon according to the schedule\n",
        "        epsilon_scheduler.set_epsilon(step=frame/params.batch_mul)\n",
        "\n",
        "        # Push the experience to the MQ\n",
        "        exp_queue.put(exp)\n",
        "        \n",
        "        # Publish also final statistics for every episode\n",
        "        for reward, steps in exp_source.pop_rewards_steps():\n",
        "            exp_queue.put(EpisodeEnded(reward, steps, selector.epsilon))"
      ],
      "id": "jFM2XcHe8ItU",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "minus-regular"
      },
      "source": [
        "## Training Loop"
      ],
      "id": "minus-regular"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lonely-sunset"
      },
      "source": [
        ""
      ],
      "id": "lonely-sunset",
      "execution_count": null,
      "outputs": []
    }
  ]
}