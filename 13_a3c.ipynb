{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-ordinary",
   "metadata": {},
   "source": [
    "# Asynchronous Advantage Actor-Critic\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/matyama/deep-rl-hands-on/blob/main/13_a3c.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "        Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "overall-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "\n",
    "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
    "pip install ptan>=0.7 tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-cyprus",
   "metadata": {},
   "source": [
    "## Correlation and Sample Efficiency\n",
    "Let's do a brief summary of both the value-based and policy-based methods.\n",
    "\n",
    "**Value-based methods (DQN & extensions)**\n",
    "* Off-policy methods that use different policy to sample the envrinment than it's being trained\n",
    "* Therefore can use old experiences to approximate state-action values and collect them in an experience replay buffer\n",
    "* So one particular experience will be used few times before it's retired from the replay buffer\n",
    "* For this reason it's *sample efficient*, meaning that we dont' have to interact with the environment frequently\n",
    "* And as such it's suitable in situation when such interation is costly\n",
    "\n",
    "**Policy-based methods (PG & Actor-Critic)**\n",
    "* A2C and related are on-policy methods that use the same policy to sample the environment and training\n",
    "* In this case one can't collect old experiences because these would yield a gradient for the old policy that sampled them\n",
    "* Instead, these methods use multiple concurrent environments to break sample correlations that invalidate the i.i.d. assumption of SGD methods\n",
    "* The benefit is inherent parallelism and modelling the policy directly as a distribution over actions\n",
    "* On the other hand, because each experience can be used only once, these methods are *sample inefficient* and are typically used only when the environment interation is cheap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-constitution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
