{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handmade-virginia",
   "metadata": {},
   "source": [
    "# Asynchronous Advantage Actor-Critic\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/matyama/deep-rl-hands-on/blob/main/13_a3c.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "        Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "textile-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "\n",
    "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
    "pip install ptan>=0.7 tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-homework",
   "metadata": {},
   "source": [
    "## Correlation and Sample Efficiency\n",
    "Let's do a brief summary of both the value-based and policy-based methods.\n",
    "\n",
    "**Value-based methods (DQN & extensions)**\n",
    "* Off-policy methods that use different policy to sample the envrinment than it's being trained\n",
    "* Therefore can use old experiences to approximate state-action values and collect them in an experience replay buffer\n",
    "* So one particular experience will be used few times before it's retired from the replay buffer\n",
    "* For this reason it's *sample efficient*, meaning that we dont' have to interact with the environment frequently\n",
    "* And as such it's suitable in situation when such interation is costly\n",
    "\n",
    "**Policy-based methods (PG & Actor-Critic)**\n",
    "* A2C and related are on-policy methods that use the same policy to sample the environment and training\n",
    "* In this case one can't collect old experiences because these would yield a gradient for the old policy that sampled them\n",
    "* Instead, these methods use multiple concurrent environments to break sample correlations that invalidate the i.i.d. assumption of SGD methods\n",
    "* The benefit is inherent parallelism and modelling the policy directly as a distribution over actions\n",
    "* On the other hand, because each experience can be used only once, these methods are *sample inefficient* and are typically used only when the environment interation is cheap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-prize",
   "metadata": {},
   "source": [
    "## A3C Method\n",
    "As mentioned before, A2C uses samples from multiple environments to break corellations. It, however, does it in an sequential (round-robin) and fully synchronous manner. Contrary to that, A3C introduces multiple parallel agents, each with its own NN copy, that:\n",
    "* In parallel sample the one or few environments\n",
    "* Push data to the training process running a SGD-like optimization\n",
    "* Receives upadted NN parameters back from the traininer\n",
    "\n",
    "Now, there are two basic variants depending on what the \"data\" definition is:\n",
    "* **Data parallelism** - The data produces by each agent are simply experienced transisions\n",
    "* **Gradient parallelism** - Extends the resposibility of an agent which in this setup additionally computes the loss (forward pass) and gradients (backpropagation). Gradients are then collected by the trainer to be applied in the optimization step.\n",
    "\n",
    "The difference might seem subtle but the forward pass and backpropagation (loss and gradient computation) is the most costly part of an SGD update so pushing these to the agent processes might significantly increase throughput especially if we have multiple GPUs already distributed in a cluster. If, however, we have just one beefy GPU the *data parallelism* approach is usually better and also easier to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-pitch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
