{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "08_dqn_extensions.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawaiian-reporter"
      },
      "source": [
        "# DQN Extenstions"
      ],
      "id": "hawaiian-reporter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc7akLCa4zrz",
        "outputId": "a6945b43-7015-4a81-d8c9-5dae90c5a675",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
        "\n",
        "pip install ptan>=0.7 pytorch-ignite"
      ],
      "id": "Uc7akLCa4zrz",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on Google Colab, therefore installing dependencies...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concerned-comedy"
      },
      "source": [
        "## Common Parts"
      ],
      "id": "concerned-comedy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hollow-preparation"
      },
      "source": [
        "### Imports and Hyperparameters"
      ],
      "id": "hollow-preparation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "headed-combination"
      },
      "source": [
        "# flake8: noqa: E402,I001\n",
        "\n",
        "import random\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from types import SimpleNamespace\n",
        "from typing import Any, Collection, Dict, Iterable, List, NamedTuple, Union\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import ptan\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
        "from ignite.engine import Engine\n",
        "from ignite.metrics import RunningAverage\n",
        "from ptan.actions import EpsilonGreedyActionSelector\n",
        "from ptan.agent import DQNAgent, TargetNet\n",
        "from ptan.experience import (\n",
        "    ExperienceFirstLast,\n",
        "    ExperienceReplayBuffer,\n",
        "    ExperienceSource,\n",
        "    ExperienceSourceFirstLast,\n",
        "    ExperienceSourceRollouts,\n",
        ")\n",
        "from ptan.ignite import (\n",
        "    EndOfEpisodeHandler,\n",
        "    EpisodeEvents,\n",
        "    EpisodeFPSHandler,\n",
        "    PeriodEvents,\n",
        "    PeriodicEvents,\n",
        ")\n",
        "\n",
        "ExpSource = Union[\n",
        "    ExperienceSource,\n",
        "    ExperienceSourceFirstLast,\n",
        "    ExperienceSourceRollouts,\n",
        "]\n",
        "\n",
        "\n",
        "# RNG seed\n",
        "SEED = 123\n",
        "\n",
        "\n",
        "# Hyperparameters for different Atari games\n",
        "HYPERPARAMS = {\n",
        "    \"pong\": SimpleNamespace(\n",
        "        **{\n",
        "            \"env_name\": \"PongNoFrameskip-v4\",\n",
        "            \"stop_reward\": 18.0,\n",
        "            \"run_name\": \"pong\",\n",
        "            \"log_period\": 10,\n",
        "            \"replay_size\": 100000,\n",
        "            \"replay_initial\": 10000,\n",
        "            \"target_net_sync\": 1000,\n",
        "            \"epsilon_frames\": 10 ** 5,\n",
        "            \"epsilon_start\": 1.0,\n",
        "            \"epsilon_final\": 0.02,\n",
        "            \"learning_rate\": 0.0001,\n",
        "            \"gamma\": 0.99,\n",
        "            \"batch_size\": 32,\n",
        "        }\n",
        "    ),\n",
        "    \"breakout-small\": SimpleNamespace(\n",
        "        **{\n",
        "            \"env_name\": \"BreakoutNoFrameskip-v4\",\n",
        "            \"stop_reward\": 500.0,\n",
        "            \"run_name\": \"breakout-small\",\n",
        "            \"log_period\": 10,\n",
        "            \"replay_size\": 3 * 10 ** 5,\n",
        "            \"replay_initial\": 20000,\n",
        "            \"target_net_sync\": 1000,\n",
        "            \"epsilon_frames\": 10 ** 6,\n",
        "            \"epsilon_start\": 1.0,\n",
        "            \"epsilon_final\": 0.1,\n",
        "            \"learning_rate\": 0.0001,\n",
        "            \"gamma\": 0.99,\n",
        "            \"batch_size\": 64,\n",
        "        }\n",
        "    ),\n",
        "    \"breakout\": SimpleNamespace(\n",
        "        **{\n",
        "            \"env_name\": \"BreakoutNoFrameskip-v4\",\n",
        "            \"stop_reward\": 500.0,\n",
        "            \"run_name\": \"breakout\",\n",
        "            \"log_period\": 10,\n",
        "            \"replay_size\": 10 ** 6,\n",
        "            \"replay_initial\": 50000,\n",
        "            \"target_net_sync\": 10000,\n",
        "            \"epsilon_frames\": 10 ** 6,\n",
        "            \"epsilon_start\": 1.0,\n",
        "            \"epsilon_final\": 0.1,\n",
        "            \"learning_rate\": 0.00025,\n",
        "            \"gamma\": 0.99,\n",
        "            \"batch_size\": 32,\n",
        "        }\n",
        "    ),\n",
        "    \"invaders\": SimpleNamespace(\n",
        "        **{\n",
        "            \"env_name\": \"SpaceInvadersNoFrameskip-v4\",\n",
        "            \"stop_reward\": 500.0,\n",
        "            \"run_name\": \"breakout\",\n",
        "            \"log_period\": 10,\n",
        "            \"replay_size\": 10 ** 6,\n",
        "            \"replay_initial\": 50000,\n",
        "            \"target_net_sync\": 10000,\n",
        "            \"epsilon_frames\": 10 ** 6,\n",
        "            \"epsilon_start\": 1.0,\n",
        "            \"epsilon_final\": 0.1,\n",
        "            \"learning_rate\": 0.00025,\n",
        "            \"gamma\": 0.99,\n",
        "            \"batch_size\": 32,\n",
        "        }\n",
        "    ),\n",
        "}"
      ],
      "id": "headed-combination",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seasonal-learning"
      },
      "source": [
        "### Experience Batches"
      ],
      "id": "seasonal-learning"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brazilian-mexico"
      },
      "source": [
        "def batch_stream(\n",
        "    buffer: ExperienceReplayBuffer,\n",
        "    initial: int,\n",
        "    batch_size: int,\n",
        ") -> Iterable[List[ExperienceFirstLast]]:\n",
        "    \"\"\"\n",
        "    Fills up the buffer with `initial` capacity and then\n",
        "    produces an infinite stream of sampled batches from it\n",
        "    while adding new experiences.\n",
        "    \"\"\"\n",
        "    buffer.populate(initial)\n",
        "    while True:\n",
        "        buffer.populate(1)\n",
        "        yield buffer.sample(batch_size)\n",
        "\n",
        "\n",
        "class ExperienceBatch(NamedTuple):\n",
        "    states: np.ndarray\n",
        "    actions: np.ndarray\n",
        "    rewards: np.ndarray\n",
        "    last_states: np.ndarray\n",
        "    dones: np.ndarray\n",
        "\n",
        "\n",
        "def unpack_batch(batch: Collection[ExperienceFirstLast]) -> ExperienceBatch:\n",
        "    states, actions, rewards, last_states, dones = [], [], [], [], []\n",
        "\n",
        "    # Unzip batch experiences into components\n",
        "    #  - When epoch ends last_state is the initial state\n",
        "    #  - This is ok because the result will be masked anyway\n",
        "    for exp in batch:\n",
        "\n",
        "        done = exp.last_state is None\n",
        "        state = np.array(exp.state)\n",
        "        last_state = state if done else np.array(exp.last_state)\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(exp.action)\n",
        "        rewards.append(exp.reward)\n",
        "        last_states.append(last_state)\n",
        "        dones.append(done)\n",
        "\n",
        "    # Wrap all components into numpy and a named tuple\n",
        "    return ExperienceBatch(\n",
        "        states=np.array(states, copy=False),\n",
        "        actions=np.array(actions),\n",
        "        rewards=np.array(rewards, dtype=np.float32),\n",
        "        last_states=np.array(last_states, copy=False),\n",
        "        dones=np.array(dones, dtype=np.uint8),\n",
        "    )"
      ],
      "id": "brazilian-mexico",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "desperate-ontario"
      },
      "source": [
        "### Loss function"
      ],
      "id": "desperate-ontario"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dimensional-database"
      },
      "source": [
        "def dqn_loss(\n",
        "    batch: Collection[ExperienceFirstLast],\n",
        "    net: nn.Module,\n",
        "    target_net: TargetNet,\n",
        "    gamma: float,\n",
        "    device: str = \"cpu\",\n",
        "):\n",
        "    # Unwrap experience batch into components\n",
        "    exp_batch = unpack_batch(batch)\n",
        "\n",
        "    # Turn them into tensors\n",
        "    states = torch.tensor(exp_batch.states).to(device)\n",
        "    next_states = torch.tensor(exp_batch.last_states).to(device)\n",
        "    actions = torch.tensor(exp_batch.actions).to(device)\n",
        "    rewards = torch.tensor(exp_batch.rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(exp_batch.dones).to(device)\n",
        "\n",
        "    # Compute Q values for all actions played in batch states\n",
        "    q_values = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # Do not add these operations to the computation graph for autograd\n",
        "    with torch.no_grad():\n",
        "        # Compute fixed values of future states using the target DQN\n",
        "        #  - And zero out terminal states\n",
        "        future_values = target_net(next_states).max(1)[0]\n",
        "        future_values[done_mask] = 0.0\n",
        "\n",
        "    # Compute TD targets and their MSE to current Q values\n",
        "    td_targets = rewards + gamma * future_values.detach()\n",
        "    return nn.MSELoss()(q_values, td_targets)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_state_values(\n",
        "    states: np.ndarray,\n",
        "    net: nn.Module,\n",
        "    device: str = \"cpu\",\n",
        ") -> np.ndarray:\n",
        "    mean_values = []\n",
        "\n",
        "    for batch in np.array_split(states, 64):\n",
        "        states = torch.tensor(batch).to(device)\n",
        "        q_values = net(states)\n",
        "        best_q_values = q_values.max(1)[0]\n",
        "        mean_values.append(best_q_values.mean().item())\n",
        "\n",
        "    return np.mean(mean_values)"
      ],
      "id": "dimensional-database",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noticed-exclusion"
      },
      "source": [
        "### Epsilon Schedule"
      ],
      "id": "noticed-exclusion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scientific-locator"
      },
      "source": [
        "class EpsilonScheduler:\n",
        "    def __init__(\n",
        "        self,\n",
        "        selector: EpsilonGreedyActionSelector,\n",
        "        params: SimpleNamespace,\n",
        "    ) -> None:\n",
        "        self.selector = selector\n",
        "        self.params = params\n",
        "        self.set_epsilon(t=0)\n",
        "\n",
        "    def set_epsilon(self, t: int) -> None:\n",
        "        \"\"\"\n",
        "        Set epsilon for current iteration t in the associated selector.\n",
        "        \"\"\"\n",
        "        eps = self.params.epsilon_start - t / self.params.epsilon_frames\n",
        "        self.selector.epsilon = max(self.params.epsilon_final, eps)"
      ],
      "id": "scientific-locator",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "young-attachment"
      },
      "source": [
        "### Ignite Handlers"
      ],
      "id": "young-attachment"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eligible-plumbing"
      },
      "source": [
        "def setup_ignite(\n",
        "    engine: Engine,\n",
        "    params: SimpleNamespace,\n",
        "    exp_source: ExpSource,\n",
        "    run_name: str,\n",
        "    extra_metrics: Iterable[str] = (),\n",
        "):\n",
        "    # Get rid of missing metrics warning\n",
        "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "\n",
        "    # Register some PTAN handlers to the Ingite engine\n",
        "    handler = EndOfEpisodeHandler(\n",
        "        exp_source, bound_avg_reward=params.stop_reward\n",
        "    )\n",
        "    handler.attach(engine)\n",
        "    EpisodeFPSHandler().attach(engine)\n",
        "\n",
        "    # Log training progress at the end of each episode\n",
        "    @engine.on(EpisodeEvents.EPISODE_COMPLETED(every=params.log_period))\n",
        "    def episode_completed(trainer: Engine) -> None:\n",
        "        passed = trainer.state.metrics.get(\"time_passed\", 0)\n",
        "        elapsed = timedelta(seconds=int(passed))\n",
        "        avg_fps = trainer.state.metrics.get(\"avg_fps\", 0)\n",
        "        print(\n",
        "            f\"Episode {trainer.state.episode}: \"\n",
        "            f\"reward={trainer.state.episode_reward:.0f}, \"\n",
        "            f\"steps={trainer.state.episode_steps}, \"\n",
        "            f\"speed={avg_fps:.1f} fps, \"\n",
        "            f\"elapsed={elapsed}\"\n",
        "        )\n",
        "\n",
        "    # Log and terminate when good enough solution has been found\n",
        "    @engine.on(EpisodeEvents.BOUND_REWARD_REACHED)\n",
        "    def game_solved(trainer: Engine) -> None:\n",
        "        passed = trainer.state.metrics[\"time_passed\"]\n",
        "        elapsed = timedelta(seconds=int(passed))\n",
        "        print(\n",
        "            f\"Game solved in {elapsed}, \"\n",
        "            f\"after {trainer.state.episode} episodes \"\n",
        "            f\"and {trainer.state.iteration} iterations!\"\n",
        "        )\n",
        "        trainer.should_terminate = True\n",
        "\n",
        "    # Setup TensorBoard logger with fresh logging directory\n",
        "    now = datetime.now().isoformat(timespec=\"minutes\").replace(\":\", \"\")\n",
        "    logdir = f\"runs/{now}-{params.run_name}-{run_name}\"\n",
        "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
        "\n",
        "    # Register smoothened loss as a metric\n",
        "    run_avg = RunningAverage(output_transform=lambda v: v[\"loss\"])\n",
        "    run_avg.attach(engine, \"avg_loss\")\n",
        "\n",
        "    # Record following metrics at the end of each epoch\n",
        "    metrics = [\"reward\", \"steps\", \"avg_reward\"]\n",
        "    tb.attach(\n",
        "        engine=engine,\n",
        "        log_handler=tb_logger.OutputHandler(\n",
        "            tag=\"episodes\", metric_names=metrics\n",
        "        ),\n",
        "        event_name=EpisodeEvents.EPISODE_COMPLETED,\n",
        "    )\n",
        "\n",
        "    # Write other metrics to TensorBoard every 100 iterations\n",
        "    PeriodicEvents().attach(engine)\n",
        "\n",
        "    handler = tb_logger.OutputHandler(\n",
        "        tag=\"train\",\n",
        "        metric_names=[\"avg_loss\", \"avg_fps\"] + list(extra_metrics),\n",
        "        output_transform=lambda a: a,\n",
        "    )\n",
        "\n",
        "    tb.attach(\n",
        "        engine=engine,\n",
        "        log_handler=handler,\n",
        "        event_name=PeriodEvents.ITERS_100_COMPLETED,\n",
        "    )"
      ],
      "id": "eligible-plumbing",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiple-french"
      },
      "source": [
        "## Basic DQN\n",
        "Let's create a baseline DQN. This is the model we used in previous chapter to solve Atari Pong but re-implemented using PTAN and PyTorch Ignite. Here's a list to point out some of the distinguishing features:\n",
        "* Uses an experience replay buffer to break correlations between observations/experiences\n",
        "* Has a second \"target\" network to freeze TD targets that is periodically synchronized with the main DQN\n",
        "* Integrates with TensorBoard for training progress monitoring"
      ],
      "id": "multiple-french"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "attempted-indonesia"
      },
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape: Tuple[int, ...], n_actions: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        n_conv_inputs = input_shape[0]\n",
        "\n",
        "        # Stack of 2D convolutional layers with ReLU activations\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(n_conv_inputs, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        n_fc_inputs = self._conv_output_dim(input_shape)\n",
        "\n",
        "        # Fully connected layers for the regression part\n",
        "        #  - Outputs Q(., a) for each action a\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_fc_inputs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions),\n",
        "        )\n",
        "\n",
        "    def _conv_output_dim(self, shape: Tuple[int, ...]) -> int:\n",
        "        dummy_conv_input = torch.zeros(1, *shape, dtype=torch.float32)\n",
        "        dummy_conv_output = self.conv(dummy_conv_input)\n",
        "        return int(np.prod(dummy_conv_output.size()))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Scale the inputs\n",
        "        inputs = x.float() / 256\n",
        "        # `view(batch_size, -1)` flattens all the feature dimensions\n",
        "        conv_out = self.conv(inputs).view(inputs.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "id": "attempted-indonesia",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "subsequent-prison",
        "outputId": "2c1cdfca-782a-444c-b495-b2d9c49bfd41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set RNG state\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Determine where computations will take place\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} to run computations...\")\n",
        "\n",
        "# Get the set of hyperparameters for Atari Pong\n",
        "params = HYPERPARAMS[\"pong\"]\n",
        "\n",
        "# Setup Atari Pong environment\n",
        "env = gym.make(params.env_name)\n",
        "env = ptan.common.wrappers.wrap_dqn(env)\n",
        "env.seed(SEED)\n",
        "\n",
        "# Create main and target DQNs\n",
        "main_net = DQN(\n",
        "    input_shape=env.observation_space.shape,\n",
        "    n_actions=env.action_space.n,\n",
        ").to(device)\n",
        "\n",
        "target_net = TargetNet(main_net)\n",
        "\n",
        "# Create the DQN agent with adaptive epsilon-greedy exploration\n",
        "selector = EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
        "epsilon_scheduler = EpsilonScheduler(selector, params)\n",
        "agent = DQNAgent(main_net, selector, device=device)\n",
        "\n",
        "# Setup experience source and replay buffer\n",
        "exp_source = ExperienceSourceFirstLast(env, agent, gamma=params.gamma)\n",
        "buffer = ExperienceReplayBuffer(exp_source, buffer_size=params.replay_size)\n",
        "\n",
        "# Create an optimizer for DQN parameters\n",
        "optimizer = torch.optim.Adam(main_net.parameters(), lr=params.learning_rate)\n",
        "\n",
        "\n",
        "def process_batch(\n",
        "    engine: Engine,\n",
        "    batch: Collection[ExperienceFirstLast],\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    # Reset the optimizer for current iteration\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute the gradient of MSE between current Q values and TD targets\n",
        "    loss = dqn_loss(\n",
        "        batch=batch,\n",
        "        net=main_net,\n",
        "        target_net=target_net.target_model,\n",
        "        gamma=params.gamma,\n",
        "        device=device,\n",
        "    )\n",
        "    loss.backward()\n",
        "\n",
        "    # Do one gradinet descent step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Decay epsilon according to the schedule\n",
        "    epsilon_scheduler.set_epsilon(t=engine.state.iteration)\n",
        "\n",
        "    # Periodically synchronize the weights of main and target DQNs\n",
        "    if engine.state.iteration % params.target_net_sync == 0:\n",
        "        target_net.sync()\n",
        "\n",
        "    # Return main metrics for tracking purposess\n",
        "    return {\"loss\": loss.item(), \"epsilon\": selector.epsilon}\n",
        "\n",
        "\n",
        "# Setup Ignite engine\n",
        "engine = Engine(process_batch)\n",
        "setup_ignite(engine, params, exp_source, run_name=\"01_baseline\")\n",
        "\n",
        "# Run the training engine on the experience replay stream\n",
        "exp_stream = batch_stream(buffer, params.replay_initial, params.batch_size)\n",
        "# engine.run(exp_stream)"
      ],
      "id": "subsequent-prison",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda to run computations...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worst-electron"
      },
      "source": [
        ""
      ],
      "id": "worst-electron",
      "execution_count": null,
      "outputs": []
    }
  ]
}