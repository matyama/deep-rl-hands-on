{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "guided-former",
   "metadata": {},
   "source": [
    "# Cross-Entropy Method\n",
    "Let's start with several ways how one can describe a RL method:\n",
    "* **model-free** vs **model-based**: Does the method construct a model of the environment or does it simply associate obervations with appropriate actions?\n",
    "* **policy-based** vs **value-based**: Does it directly learn a policy or a value function and then picks an action that maximizes this value?\n",
    "* **off-policy** vs **on-policy**: Does it work based on historical data collected either by previous version, the same agent several episodes ago or a human (off-policy) or does require fresh observations (on-policy)?\n",
    "\n",
    "The *Cross-Entropy Method* is a *model-free* (does not build a model of the environment), *policy-based* (approximates the policy), *on-policy* (requires freas data from the environment) method that even though being quite simple works pretty well in non-complex envirnments where episodes are expected to be short. \n",
    "\n",
    "We represent the agent by a NN (or any other classifier) that takes observations on the input and outputs class probabilities for each action (the policy). The outline of the method is as follows:\n",
    "1. We let the agent play N episodes in the environment with current model\n",
    "1. Then we compute the total (discounted) reward for each episode and drop those below a reward boundary (e.g. 70th percentile)\n",
    "1. Next we train the NN on a batch of the remaining episodes with observations as inputs and corresponding actions as targets (i.e. fit the model to produce actions that lead high rewards)\n",
    "1. go to 1. or terminate if the policy is good enough or we ran out of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-pencil",
   "metadata": {},
   "source": [
    "## CartPole\n",
    "Let's use the Cross-Entropy method on the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floral-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, NamedTuple, Optional, Sequence, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Maximum total reward in CartPole\n",
    "MAX_REWARD = 200\n",
    "\n",
    "# Hyperparameters\n",
    "HIDDEN_UNITS = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple network representing a policy.\n",
    "\n",
    "    Note that the actual output of this NN are raw action scores\n",
    "    and to convert them to class probabilities one should use a softmax.\n",
    "    For training we'll use the `nn.CrossEntropyLoss` to cover for this\n",
    "    and to get better numerical stability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        n_hidden: int,\n",
    "        n_actions: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TimeStep(NamedTuple):\n",
    "    observation: np.ndarray\n",
    "    action: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Episode:\n",
    "    reward: float\n",
    "    steps: List[TimeStep]\n",
    "\n",
    "    @classmethod\n",
    "    def new(cls) -> \"Episode\":\n",
    "        return cls(reward=0.0, steps=[])\n",
    "\n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        return len(self.steps)\n",
    "\n",
    "\n",
    "def iterate_batches(\n",
    "    env: gym.Env,\n",
    "    net: PolicyNet,\n",
    "    batch_size: int,\n",
    ") -> Iterable[List[Episode]]:\n",
    "\n",
    "    # Make a softmax layer for computing action probabilities\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def sample_action(obs: np.ndarray) -> int:\n",
    "        action_probs = softmax(net(torch.FloatTensor([obs])))\n",
    "        action_probs = action_probs.data.numpy()[0]\n",
    "        return np.random.choice(len(action_probs), p=action_probs)\n",
    "\n",
    "    # Current batch and episode\n",
    "    batch = []\n",
    "    episode = Episode.new()\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Sample an action from the policy net\n",
    "        #  - and apply it to the environment\n",
    "        action = sample_action(obs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Add new step to current episode\n",
    "        episode.reward += reward\n",
    "        episode.steps.append(TimeStep(obs, action))\n",
    "\n",
    "        if done:\n",
    "            # Record old and start new episode\n",
    "            batch.append(episode)\n",
    "            episode = Episode.new()\n",
    "            next_obs = env.reset()\n",
    "\n",
    "            # Output new batch if current one is of full size\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(\n",
    "    batch: Sequence[Episode],\n",
    "    percentile: float,\n",
    "    gamma: Optional[float] = None,\n",
    ") -> Tuple[List[Episode], torch.FloatTensor, torch.LongTensor, float, float]:\n",
    "\n",
    "    if gamma is not None and 0 < gamma <= 1:\n",
    "        rewards = [e.reward * (gamma ** e.length) for e in batch]\n",
    "    else:\n",
    "        rewards = [e.reward for e in batch]\n",
    "\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = np.mean(rewards)\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    elite_batch = []\n",
    "\n",
    "    for episode, total_reward in zip(batch, rewards):\n",
    "\n",
    "        # Check if this episode is good enough\n",
    "        if total_reward >= reward_bound:\n",
    "\n",
    "            # Collect observations and actions from the episode\n",
    "            for observation, action in episode.steps:\n",
    "                observations.append(observation)\n",
    "                actions.append(action)\n",
    "\n",
    "            elite_batch.append(episode)\n",
    "\n",
    "    observations = torch.FloatTensor(observations)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    return elite_batch, observations, actions, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ideal-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: loss=0.536, r_mean=200.0, r_bound=200.0\r"
     ]
    }
   ],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Build the policy net\n",
    "net = PolicyNet(obs_dim, HIDDEN_UNITS, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "\n",
    "with SummaryWriter(comment=\"-cartpole\") as writer:\n",
    "\n",
    "    for i, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Find tbe best episodes and collect their observations and actions\n",
    "        _, obs, actions, r_bound, r_mean = filter_batch(batch, PERCENTILE)\n",
    "\n",
    "        # Use the net to compute action scores for episodic observations\n",
    "        action_scores = net(obs)\n",
    "\n",
    "        # Compute the cross-entropy loss and gradients\n",
    "        #  - We use all the episodic actions as targets\n",
    "        loss = objective(action_scores, actions)\n",
    "        loss.backward()\n",
    "\n",
    "        # Make a gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the progress\n",
    "        print(\n",
    "            f\"{i}: loss={loss.item():.3f}, \"\n",
    "            f\"r_mean={r_mean:.1f}, r_bound={r_bound:.1f}\",\n",
    "            end=\"\\r\",\n",
    "        )\n",
    "\n",
    "        # Record current statistics\n",
    "        writer.add_scalar(\"loss\", loss.item(), i)\n",
    "        writer.add_scalar(\"reward_bound\", r_bound, i)\n",
    "        writer.add_scalar(\"reward_mean\", r_mean, i)\n",
    "\n",
    "        # Stop if we can expect maximum possible using this policy\n",
    "        if r_mean >= MAX_REWARD:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-gazette",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
