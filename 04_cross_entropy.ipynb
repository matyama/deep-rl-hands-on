{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convertible-michael",
   "metadata": {},
   "source": [
    "# Cross-Entropy Method\n",
    "Let's start with several ways how one can describe a RL method:\n",
    "* **model-free** vs **model-based**: Does the method construct a model of the environment or does it simply associate obervations with appropriate actions?\n",
    "* **policy-based** vs **value-based**: Does it directly learn a policy or a value function and then picks an action that maximizes this value?\n",
    "* **off-policy** vs **on-policy**: Does it work based on historical data collected either by previous version, the same agent several episodes ago or a human (off-policy) or does require fresh observations (on-policy)?\n",
    "\n",
    "The *Cross-Entropy Method* is a *model-free* (does not build a model of the environment), *policy-based* (approximates the policy), *on-policy* (requires freas data from the environment) method that even though being quite simple works pretty well in non-complex envirnments where episodes are expected to be short. \n",
    "\n",
    "We represent the agent by a NN (or any other classifier) that takes observations on the input and outputs class probabilities for each action (the policy). The outline of the method is as follows:\n",
    "1. We let the agent play N episodes in the environment with current model\n",
    "1. Then we compute the total (discounted) reward for each episode and drop those below a reward boundary (e.g. 70th percentile)\n",
    "1. Next we train the NN on a batch of the remaining episodes with observations as inputs and corresponding actions as targets (i.e. fit the model to produce actions that lead high rewards)\n",
    "1. go to 1. or terminate if the policy is good enough or we ran out of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-aviation",
   "metadata": {},
   "source": [
    "## CartPole\n",
    "Let's use the Cross-Entropy method on the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "native-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, NamedTuple, Sequence, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Maximum total reward in CartPole\n",
    "MAX_REWARD = 200\n",
    "\n",
    "# Hyperparameters\n",
    "HIDDEN_UNITS = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple network representing a policy.\n",
    "\n",
    "    Note that the actual output of this NN are raw action scores\n",
    "    and to convert them to class probabilities one should use a softmax.\n",
    "    For training we'll use the `nn.CrossEntropyLoss` to cover for this\n",
    "    and to get better numerical stability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        n_hidden: int,\n",
    "        n_actions: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TimeStep(NamedTuple):\n",
    "    observation: np.ndarray\n",
    "    action: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Episode:\n",
    "    reward: float\n",
    "    discounted_reward: float\n",
    "    steps: List[TimeStep]\n",
    "\n",
    "    @classmethod\n",
    "    def new(cls) -> \"Episode\":\n",
    "        return cls(reward=0.0, discounted_reward=0.0, steps=[])\n",
    "\n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        return len(self.steps)\n",
    "\n",
    "\n",
    "def iterate_batches(\n",
    "    env: gym.Env, net: PolicyNet, batch_size: int, gamma: float = 1.0\n",
    ") -> Iterable[List[Episode]]:\n",
    "    assert 0 < gamma <= 1\n",
    "    discount_rewards = gamma < 1\n",
    "\n",
    "    # Make a softmax layer for computing action probabilities\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def sample_action(obs: np.ndarray) -> int:\n",
    "        action_probs = softmax(net(torch.FloatTensor([obs])))\n",
    "        action_probs = action_probs.data.numpy()[0]\n",
    "        return np.random.choice(len(action_probs), p=action_probs)\n",
    "\n",
    "    # Current batch and episode\n",
    "    batch = []\n",
    "    episode = Episode.new()\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Sample an action from the policy net\n",
    "        #  - and apply it to the environment\n",
    "        action = sample_action(obs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        discounted_reward = reward\n",
    "\n",
    "        # Discount reward if necessary\n",
    "        if discount_rewards:\n",
    "            discounted_reward *= gamma ** episode.length\n",
    "\n",
    "        # Add new step to current episode\n",
    "        episode.reward += reward\n",
    "        episode.discounted_reward += discounted_reward\n",
    "        episode.steps.append(TimeStep(obs, action))\n",
    "\n",
    "        if done:\n",
    "            # Record old and start new episode\n",
    "            batch.append(episode)\n",
    "            episode = Episode.new()\n",
    "            next_obs = env.reset()\n",
    "\n",
    "            # Output new batch if current one is of full size\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(\n",
    "    batch: Sequence[Episode],\n",
    "    percentile: float,\n",
    ") -> Tuple[List[Episode], torch.FloatTensor, torch.LongTensor, float, float]:\n",
    "\n",
    "    # Collect discounted rewards and compute new reward bound\n",
    "    #  - Also compute the mean for monitoring\n",
    "    discounted_rewards = [e.discounted_reward for e in batch]\n",
    "    reward_bound = np.percentile(discounted_rewards, percentile)\n",
    "    reward_mean = np.mean([e.reward for e in batch])\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    elite_batch = []\n",
    "\n",
    "    for episode, reward in zip(batch, discounted_rewards):\n",
    "\n",
    "        # Check if this episode is good enough\n",
    "        if reward >= reward_bound:\n",
    "\n",
    "            # Collect observations and actions from the episode\n",
    "            for observation, action in episode.steps:\n",
    "                observations.append(observation)\n",
    "                actions.append(action)\n",
    "\n",
    "            elite_batch.append(episode)\n",
    "\n",
    "    observations = torch.FloatTensor(observations)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    return elite_batch, observations, actions, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "precious-sensitivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42: loss=0.526, r_mean=200.0, r_bound=200.0\r"
     ]
    }
   ],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Build the policy net\n",
    "net = PolicyNet(obs_dim, HIDDEN_UNITS, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "\n",
    "with SummaryWriter(comment=\"-cartpole\") as writer:\n",
    "\n",
    "    for i, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Find tbe best episodes and collect their observations and actions\n",
    "        _, obs, actions, r_bound, r_mean = filter_batch(batch, PERCENTILE)\n",
    "\n",
    "        # Use the net to compute action scores for episodic observations\n",
    "        action_scores = net(obs)\n",
    "\n",
    "        # Compute the cross-entropy loss and gradients\n",
    "        #  - We use all the episodic actions as targets\n",
    "        loss = objective(action_scores, actions)\n",
    "        loss.backward()\n",
    "\n",
    "        # Make a gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the progress\n",
    "        print(\n",
    "            f\"{i}: loss={loss.item():.3f}, \"\n",
    "            f\"r_mean={r_mean:.1f}, r_bound={r_bound:.1f}\",\n",
    "            end=\"\\r\",\n",
    "        )\n",
    "\n",
    "        # Record current statistics\n",
    "        writer.add_scalar(\"loss\", loss.item(), i)\n",
    "        writer.add_scalar(\"reward_bound\", r_bound, i)\n",
    "        writer.add_scalar(\"reward_mean\", r_mean, i)\n",
    "\n",
    "        # Stop if we can expect maximum possible using this policy\n",
    "        if r_mean >= MAX_REWARD:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-sheep",
   "metadata": {},
   "source": [
    "## FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-episode",
   "metadata": {},
   "source": [
    "### Naive Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unusual-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: loss=0.001, r_mean=0.0, r_bound=0.0\r"
     ]
    }
   ],
   "source": [
    "MAX_REWARD = 0.8\n",
    "MAX_ITERS = 1000\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Converts each discrete observation to a 1-hot encoded vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0,\n",
    "            high=1.0,\n",
    "            shape=(env.observation_space.n,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "# Create FrozenLake environment\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Build the policy net\n",
    "net = PolicyNet(obs_dim, HIDDEN_UNITS, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "\n",
    "with SummaryWriter(comment=\"-frozenlake-naive\") as writer:\n",
    "\n",
    "    for i, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Find tbe best episodes and collect their observations and actions\n",
    "        _, obs, actions, r_bound, r_mean = filter_batch(batch, PERCENTILE)\n",
    "\n",
    "        # Use the net to compute action scores for episodic observations\n",
    "        action_scores = net(obs)\n",
    "\n",
    "        # Compute the cross-entropy loss and gradients\n",
    "        #  - We use all the episodic actions as targets\n",
    "        loss = objective(action_scores, actions)\n",
    "        loss.backward()\n",
    "\n",
    "        # Make a gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the progress\n",
    "        print(\n",
    "            f\"{i}: loss={loss.item():.3f}, \"\n",
    "            f\"r_mean={r_mean:.1f}, r_bound={r_bound:.1f}\",\n",
    "            end=\"\\r\",\n",
    "        )\n",
    "\n",
    "        # Record current statistics\n",
    "        writer.add_scalar(\"loss\", loss.item(), i)\n",
    "        writer.add_scalar(\"reward_bound\", r_bound, i)\n",
    "        writer.add_scalar(\"reward_mean\", r_mean, i)\n",
    "\n",
    "        # Stop if we can expect maximum possible using this policy\n",
    "        if r_mean >= MAX_REWARD or i == MAX_ITERS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-nigeria",
   "metadata": {},
   "source": [
    "### Improved Version\n",
    "There are several reasons why the training failed on FrozenLake (mean reward didn't imrove at all). All these reasons point out the disadvantages of the Cross-Entropy method which are:\n",
    "* FrozenLake has just two rewards - 0 for failure and 1 after reaching the goal state so the episodic reward distribution is far from normal.\n",
    "* Moreover, because there is no diversity in reward values, our reward boundary does not work (it can easily happen that most episodes end with reward 0)\n",
    "* Furthermore, the reward is very delayed (credit assignment problem)\n",
    "\n",
    "What we can do to improve the method is following:\n",
    "1. Discount future rewards in the episode (in this case basically just the last one)\n",
    "1. Keep a small buffer of historical elite episodes and use them for next filtering\n",
    "1. Increase batch size and decrease the learning rate\n",
    "1. Longer learning time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "matched-petite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: loss=0.339, r_mean=0.0, r_bound=0.0\r"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "random.seed(12345)\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_REWARD = 0.8\n",
    "MAX_ITERS = 10_000\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9\n",
    "LEARNING_RATE = 0.001\n",
    "HISTORY_SIZE = 500\n",
    "\n",
    "# Create FrozenLake environment\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Build the policy net\n",
    "net = PolicyNet(obs_dim, HIDDEN_UNITS, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "with SummaryWriter(comment=\"-frozenlake-improved\") as writer:\n",
    "\n",
    "    elite = []\n",
    "\n",
    "    i = 0\n",
    "    for batch in iterate_batches(env, net, BATCH_SIZE, GAMMA):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extend the batch with the history of elite episodes\n",
    "        batch = elite + batch\n",
    "\n",
    "        # Find tbe best episodes and collect their observations and actions\n",
    "        elite, obs, actions, r_bound, r_mean = filter_batch(batch, PERCENTILE)\n",
    "\n",
    "        if not elite:\n",
    "            # Do not apply (and count) this learning step\n",
    "            continue\n",
    "\n",
    "        # Limit the history of elite episodes\n",
    "        elite = elite[-HISTORY_SIZE:]\n",
    "\n",
    "        # Use the net to compute action scores for episodic observations\n",
    "        action_scores = net(obs)\n",
    "\n",
    "        # Compute the cross-entropy loss and gradients\n",
    "        #  - We use all the episodic actions as targets\n",
    "        loss = objective(action_scores, actions)\n",
    "        loss.backward()\n",
    "\n",
    "        # Make a gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the progress\n",
    "        print(\n",
    "            f\"{i}: loss={loss.item():.3f}, \"\n",
    "            f\"r_mean={r_mean:.1f}, r_bound={r_bound:.1f}\",\n",
    "            end=\"\\r\",\n",
    "        )\n",
    "\n",
    "        # Record current statistics\n",
    "        writer.add_scalar(\"loss\", loss.item(), i)\n",
    "        writer.add_scalar(\"reward_bound\", r_bound, i)\n",
    "        writer.add_scalar(\"reward_mean\", r_mean, i)\n",
    "\n",
    "        # Stop if we can expect maximum possible using this policy\n",
    "        if r_mean >= MAX_REWARD or i == MAX_ITERS:\n",
    "            break\n",
    "\n",
    "        # Increment learning step only after model update\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-adams",
   "metadata": {},
   "source": [
    "Still not great, that only shows that we might have better luck with other methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-anime",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
