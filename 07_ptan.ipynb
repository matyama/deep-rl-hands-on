{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nonprofit-southwest",
   "metadata": {},
   "source": [
    "# High-Level RL Libraries\n",
    "* [**PTAN**](https://github.com/Shmuma/ptan) - based on PyTorch, described below\n",
    "* [Keras-RL](https://github.com/keras-rl/keras-rl) - based on Keras, includes basic RL methods\n",
    "* [TF-Agents](https://www.tensorflow.org/agents) - based on TensorFlow, made by Google in 2018\n",
    "* [Dopamine](https://github.com/google/dopamine) - made by Google in 2018, TensorFlow specific\n",
    "* [Ray](https://ray.io/) - library for distributed execution of ML code including a RL library\n",
    "* [ReAgent](https://reagent.ai/) - made by Facebook, based on PyTorch and JSON problem description\n",
    "* [CatalystRL](https://github.com/catalyst-team/catalyst-rl) - uses PyTorch backend\n",
    "* [SLM Lab](https://github.com/kengz/SLM-Lab) - another RL library\n",
    "\n",
    "Following sections describe the API and basic components of the *PTAN* library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-teach",
   "metadata": {},
   "source": [
    "## Action Selectors\n",
    "An `ActionSelector` is a class that given a network output (Q values), or a batch thereof, selects a actions to play. There are three basic types:\n",
    "* *Argmax* is the standard selector used in Q-Learning\n",
    "* *Policy-based* samples actions from a given normalized distribution\n",
    "* *Epsilon Greedy* implements the epsilon-greedy strategy given an epsilon hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fleet-soccer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 1, -1,  0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ptan\n",
    "\n",
    "# Let's assume we got a batch of Q values from a NN\n",
    "q_vals = np.array([[1, 2, 3], [1, -1, 0]])\n",
    "q_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-freight",
   "metadata": {},
   "source": [
    "### Argmax Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preliminary-performer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-saudi",
   "metadata": {},
   "source": [
    "### Epsilon-greedy Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "functional-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.0)\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interstate-connection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.epsilon = 1.0\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "immediate-grave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.epsilon = 0.5\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tutorial-direction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.epsilon = 0.1\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-roulette",
   "metadata": {},
   "source": [
    "### Probability Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pressing-corruption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n",
      "[1 2 1]\n",
      "[1 2 1]\n",
      "[1 2 1]\n",
      "[1 2 0]\n",
      "[1 2 1]\n",
      "[1 2 0]\n",
      "[2 2 1]\n",
      "[1 2 0]\n",
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# A batch of probability distributions over actions\n",
    "distributions = np.array(\n",
    "    [\n",
    "        [0.1, 0.8, 0.1],\n",
    "        [0.0, 0.0, 1.0],\n",
    "        [0.5, 0.5, 0.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "\n",
    "for _ in range(10):\n",
    "    # Each time sample actions from each distribution\n",
    "    actions = selector(distributions)\n",
    "    print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-atlas",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "The main purpose of the agent class is to convert observations to actions. Actually, the input (output) is a batch of observations (actions) for better GPU utitlization by the underlying NN.\n",
    "\n",
    "There are two main classes of agents:\n",
    "* `DQNAgent` is an agent based on a DQN that makes its decisions based on action values (Q values)\n",
    "* `PolicyAgent` samples actions from a normalized probability distribution (policy) over a discrete set of actions or logits (non-normalized distribution; preferred for better numerical stability)\n",
    "\n",
    "Finally, a typical scenario is to implement custom agent by extending some of these classes (or rather the base class). The reason might be for instance:\n",
    "* Agent's decision making is not fully determined by current observation but rather a history so we need to keep an internal state (e.g. beliefs in POMDPs)\n",
    "* The NN has multi-modal inputs - e.g. both text and image pixels\n",
    "* The agent uses non-standard exploration strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-trustee",
   "metadata": {},
   "source": [
    "### DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grave-performance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # noqa\n",
    "import torch.nn as nn  # noqa\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Produces diagonal tensor of shape (batch_size, n_actions)\"\"\"\n",
    "        batch_size = x.size()[0]\n",
    "        return torch.eye(batch_size, self.n_actions)\n",
    "\n",
    "\n",
    "dqn = DQN(n_actions=3)\n",
    "\n",
    "# Try out the DQN\n",
    "dqn(torch.zeros(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corresponding-election",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), [None, None])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DQN agent using a greedy policy\n",
    "dqn_agent = ptan.agent.DQNAgent(\n",
    "    dqn_model=dqn,\n",
    "    action_selector=ptan.actions.ArgmaxActionSelector(),\n",
    ")\n",
    "\n",
    "# Test the greedy DQN agent\n",
    "dqn_agent(torch.zeros(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "impossible-roots",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 1, 1, 0, 2, 1, 2, 0]),\n",
       " [None, None, None, None, None, None, None, None, None, None])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "\n",
    "# Create a DQN agent using an epsilon-greedy policy\n",
    "dqn_agent = ptan.agent.DQNAgent(dqn_model=dqn, action_selector=selector)\n",
    "\n",
    "# Test the epsilon-greedy DQN agent\n",
    "dqn_agent(torch.zeros(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "super-feeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 1, 2, 0, 2, 1, 0, 0, 0, 0]),\n",
       " [None, None, None, None, None, None, None, None, None, None])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.epsilon = 0.5\n",
    "dqn_agent(torch.zeros(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "double-disclosure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 0, 0, 0, 0, 0, 0, 0]),\n",
       " [None, None, None, None, None, None, None, None, None, None])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.epsilon = 0.1\n",
    "dqn_agent(torch.zeros(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-suicide",
   "metadata": {},
   "source": [
    "### PolicyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "enabling-milton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_actions: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Produces a tensor with first two actions having the same logit scores\n",
    "        \"\"\"\n",
    "        batch_size = x.size()[0]\n",
    "        output = torch.zeros(\n",
    "            (batch_size, self.n_actions),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        output[:, 0] = 1\n",
    "        output[:, 1] = 1\n",
    "        return output\n",
    "\n",
    "\n",
    "policy_net = PolicyNet(n_actions=5)\n",
    "\n",
    "# Try the policy network\n",
    "policy_net(torch.zeros(6, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unexpected-joyce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 1, 1, 4, 3, 3]), [None, None, None, None, None, None])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a policy-based agent\n",
    "#  - Note: The agent uses softmax to convert the NN output to probabilities\n",
    "policy_agent = ptan.agent.PolicyAgent(\n",
    "    model=policy_net,\n",
    "    action_selector=ptan.actions.ProbabilityActionSelector(),\n",
    "    apply_softmax=True,\n",
    ")\n",
    "\n",
    "# Test the policy agent\n",
    "policy_agent(torch.zeros(6, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-bruce",
   "metadata": {},
   "source": [
    "## Experience Sources\n",
    "The philosophy behind *experience sources* is to provide an easy-to-use wrapper for handling trajectories (basically n steps of experiences). The responsibility of an experiences source is to provide trajectories given an agent and (multiple) environments.\n",
    "\n",
    "There are three basic classes:\n",
    "* `ExperienceSource` can operate over multiple environments at once (to create batches for GPU processing) and returns n-step subtrajectories with all intermediate steps\n",
    "* `ExperienceSourceFirstLast` returns subtrajectories with just the first and last step (with accumulated reward) - saves a lot of memory for n-step DQN or in A2C rollouts\n",
    "* `ExperienceSourceRollouts` follows the A3C rollouts scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alpine-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.reset() -> 0\n",
      "env.step(1) -> (0, 1.0, False, {})\n",
      "env.step(2) -> (1, 2.0, False, {})\n",
      "(2, 0.0, False, {})\n",
      "(3, 0.0, False, {})\n",
      "(4, 0.0, False, {})\n",
      "(0, 0.0, False, {})\n",
      "(1, 0.0, False, {})\n",
      "(2, 0.0, False, {})\n",
      "(3, 0.0, False, {})\n",
      "(4, 0.0, False, {})\n",
      "(0, 0.0, True, {})\n",
      "(0, 0.0, True, {})\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple  # noqa\n",
    "\n",
    "import gym  # noqa\n",
    "\n",
    "\n",
    "class ToyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment with observation 0..4 and actions 0..2\n",
    "      * Observations are rotated sequentialy mod 5\n",
    "      * Reward is equal to given action.\n",
    "      * Episodes are having fixed length of 10\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        self.step_index = 0\n",
    "\n",
    "    def reset(self) -> int:\n",
    "        self.step_index = 0\n",
    "        return self.step_index\n",
    "\n",
    "    def step(self, action: int) -> Tuple[int, float, bool, Dict]:\n",
    "        observation = self.step_index % self.observation_space.n\n",
    "        done = self.step_index == 10\n",
    "        reward = 0.0 if done else float(action)\n",
    "        if not done:\n",
    "            self.step_index += 1\n",
    "        return observation, reward, done, {}\n",
    "\n",
    "\n",
    "# Experiment with the toy environment\n",
    "env = ToyEnv()\n",
    "\n",
    "state = env.reset()\n",
    "print(f\"env.reset() -> {state}\")\n",
    "\n",
    "state = env.step(1)\n",
    "print(f\"env.step(1) -> {state}\")\n",
    "\n",
    "state = env.step(2)\n",
    "print(f\"env.step(2) -> {state}\")\n",
    "\n",
    "for _ in range(10):\n",
    "    print(env.step(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "present-excess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1], None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DummyAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"Agent that always returns a fixed action\"\"\"\n",
    "\n",
    "    def __init__(self, action: int) -> None:\n",
    "        self.action = action\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        observations: List[Any],\n",
    "        state: Optional[List[Any]] = None,\n",
    "    ) -> Tuple[List[int], Optional[List[Any]]]:\n",
    "        return [self.action for _ in observations], state\n",
    "\n",
    "\n",
    "# Create a dummy agent\n",
    "agent = DummyAgent(action=1)\n",
    "agent([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hydraulic-freight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=0.0, done=True))\n",
      "(Experience(state=4, action=1, reward=0.0, done=True),)\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, TypeVar  # noqa\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "def take(n: int, xs: Iterable[T]) -> Iterable[T]:\n",
    "    assert n >= 0\n",
    "    for x in xs:\n",
    "        if n == 0:\n",
    "            break\n",
    "        yield x\n",
    "        n -= 1\n",
    "\n",
    "\n",
    "# Create new experience source with single environment\n",
    "#  - There will be 2 experiences in each subtrajectory\n",
    "exp_source = ptan.experience.ExperienceSource(\n",
    "    env=ToyEnv(),\n",
    "    agent=agent,\n",
    "    steps_count=2,\n",
    ")\n",
    "\n",
    "# Print first 15 subtrajectories from the source\n",
    "for exp in take(15, exp_source):\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "subtle-toddler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Experience(state=0, action=1, reward=1.0, done=False),\n",
       " Experience(state=0, action=1, reward=1.0, done=False),\n",
       " Experience(state=1, action=1, reward=1.0, done=False),\n",
       " Experience(state=2, action=1, reward=1.0, done=False))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a subtrajectory containing four experiences\n",
    "exp_source = ptan.experience.ExperienceSource(\n",
    "    env=ToyEnv(),\n",
    "    agent=agent,\n",
    "    steps_count=4,\n",
    ")\n",
    "next(iter(exp_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "equipped-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "# Create a source that interacts with two environments at once\n",
    "exp_source = ptan.experience.ExperienceSource(\n",
    "    env=[ToyEnv(), ToyEnv()],\n",
    "    agent=agent,\n",
    "    steps_count=2,\n",
    ")\n",
    "\n",
    "# Take and show first 4 subtrajectories\n",
    "for exp in take(4, exp_source):\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "particular-crack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n"
     ]
    }
   ],
   "source": [
    "# Create an aggregating source with single environment\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env=ToyEnv(),\n",
    "    agent=agent,\n",
    "    gamma=1.0,\n",
    "    steps_count=1,\n",
    ")\n",
    "\n",
    "# Show first 10 subtrajectories from this source\n",
    "for exp in take(10, exp_source):\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-execution",
   "metadata": {},
   "source": [
    "## Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dated-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time, 4\tbatch samples:\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "Train time, 4\tbatch samples:\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n"
     ]
    }
   ],
   "source": [
    "# Crate an experience source with single environment\n",
    "#  - Uses the dummy agent and produces one experience at a time\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env=ToyEnv(),\n",
    "    agent=DummyAgent(action=1),\n",
    "    gamma=1.0,\n",
    "    steps_count=1,\n",
    ")\n",
    "\n",
    "# Create a ER buffer linked to the source with 100 exp. capacity\n",
    "replay_buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    experience_source=exp_source,\n",
    "    buffer_size=100,\n",
    ")\n",
    "\n",
    "# Sketch a usage of the ER buffer in a trainng loop\n",
    "\n",
    "min_buffer_size = 5\n",
    "batch_size = 4\n",
    "\n",
    "# Run 6 steps of a training loop\n",
    "for _ in range(6):\n",
    "\n",
    "    # Add single sample from the experience source to the ER buffer\n",
    "    replay_buffer.populate(1)\n",
    "\n",
    "    # Let the buffer fill up\n",
    "    if len(replay_buffer) < min_buffer_size:\n",
    "        continue\n",
    "\n",
    "    # Sample a batch from the ER buffer\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Log batch info\n",
    "    print(f\"Train time, {len(batch)}\\tbatch samples:\")\n",
    "    for sample in batch:\n",
    "        print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-slovakia",
   "metadata": {},
   "source": [
    "## Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "respective-planet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main net: Parameter containing:\n",
      "tensor([[ 0.3377, -0.3519, -0.3948, -0.3051, -0.1529],\n",
      "        [ 0.3760, -0.1221, -0.4275, -0.3246, -0.2839],\n",
      "        [-0.3563, -0.2029, -0.0848, -0.2980, -0.1327]], requires_grad=True)\n",
      "Target net: Parameter containing:\n",
      "tensor([[ 0.3377, -0.3519, -0.3948, -0.3051, -0.1529],\n",
      "        [ 0.3760, -0.1221, -0.4275, -0.3246, -0.2839],\n",
      "        [-0.3563, -0.2029, -0.0848, -0.2980, -0.1327]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class LinearDQN(nn.Module):\n",
    "    \"\"\"DQN with single linear layer\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(5, 3)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "# Create a DQN and a corresponding target network\n",
    "main_net = LinearDQN()\n",
    "target_net = ptan.agent.TargetNet(main_net)\n",
    "\n",
    "# Compare parameters of both networks\n",
    "print(\"Main net:\", main_net.lin.weight)\n",
    "print(\"Target net:\", target_net.target_model.lin.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-value",
   "metadata": {},
   "source": [
    "Simulate an update to the weights of the main network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "rational-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main net: Parameter containing:\n",
      "tensor([[1.3377, 0.6481, 0.6052, 0.6949, 0.8471],\n",
      "        [1.3760, 0.8779, 0.5725, 0.6754, 0.7161],\n",
      "        [0.6437, 0.7971, 0.9152, 0.7020, 0.8673]], requires_grad=True)\n",
      "Target net: Parameter containing:\n",
      "tensor([[ 0.3377, -0.3519, -0.3948, -0.3051, -0.1529],\n",
      "        [ 0.3760, -0.1221, -0.4275, -0.3246, -0.2839],\n",
      "        [-0.3563, -0.2029, -0.0848, -0.2980, -0.1327]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "main_net.lin.weight.data += 1.0\n",
    "print(\"Main net:\", main_net.lin.weight)\n",
    "print(\"Target net:\", target_net.target_model.lin.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-blend",
   "metadata": {},
   "source": [
    "Synchronize parameters of both models by copying weights from the main to the target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "constitutional-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main net: Parameter containing:\n",
      "tensor([[1.3377, 0.6481, 0.6052, 0.6949, 0.8471],\n",
      "        [1.3760, 0.8779, 0.5725, 0.6754, 0.7161],\n",
      "        [0.6437, 0.7971, 0.9152, 0.7020, 0.8673]], requires_grad=True)\n",
      "Target net: Parameter containing:\n",
      "tensor([[1.3377, 0.6481, 0.6052, 0.6949, 0.8471],\n",
      "        [1.3760, 0.8779, 0.5725, 0.6754, 0.7161],\n",
      "        [0.6437, 0.7971, 0.9152, 0.7020, 0.8673]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "target_net.sync()\n",
    "print(\"Main net:\", main_net.lin.weight)\n",
    "print(\"Target net:\", target_net.target_model.lin.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-powder",
   "metadata": {},
   "source": [
    "## PTAN CartPole Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "commercial-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_states: int,\n",
    "        n_actions: int,\n",
    "        n_hidden_units: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, n_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden_units, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x.float())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def unpack_batch(\n",
    "    batch: Iterable[ptan.experience.ExperienceFirstLast],\n",
    "    net: nn.Module,\n",
    "    gamma: float,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "\n",
    "    # Unzip experiences from the batch\n",
    "    for exp in batch:\n",
    "        done = exp.last_state is None\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(done)\n",
    "        last_states.append(exp.state if done else exp.last_state)\n",
    "\n",
    "    # Convert all to tensors\n",
    "    states = torch.tensor(states)\n",
    "    actions = torch.tensor(actions)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    last_states = torch.tensor(last_states)\n",
    "\n",
    "    # Compute Q values of all actions for all the last states\n",
    "    q_values = net(last_states)\n",
    "\n",
    "    # max_a' Q(s_last, a')\n",
    "    future_values = torch.max(q_values, dim=1)[0]\n",
    "    future_values[done_masks] = 0.0\n",
    "\n",
    "    # Returns states, actions and TD targets for the whole batch\n",
    "    return states, actions, rewards + gamma * future_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "vulnerable-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_hidden_units = 128\n",
    "target_sync_period = 10\n",
    "batch_size = 16\n",
    "buffer_capacity = 1000\n",
    "min_buffer_size = 2 * batch_size\n",
    "gamma = 0.9\n",
    "learning_rate = 1e-3\n",
    "epsilon_decay = 0.99\n",
    "solution_reward_bound = 150\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Create main and target DQNs\n",
    "main_net = DQN(n_states, n_actions, n_hidden_units)\n",
    "target_net = ptan.agent.TargetNet(main_net)\n",
    "\n",
    "# Create an optimizer linked to the parameters of the main DQN\n",
    "optimizer = torch.optim.Adam(main_net.parameters(), learning_rate)\n",
    "\n",
    "# Create DQN agent that uses epsilon-greedy exploration policy\n",
    "#  - Note: Initial epsilon is 1 but it will be decayed during training\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(\n",
    "    epsilon=1,\n",
    "    selector=ptan.actions.ArgmaxActionSelector(),\n",
    ")\n",
    "\n",
    "agent = ptan.agent.DQNAgent(main_net, selector)\n",
    "\n",
    "# Create new aggregating experience source\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=gamma)\n",
    "\n",
    "# Setup a ER replay buffer liked to the exp. source\n",
    "replay_buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    experience_source=exp_source,\n",
    "    buffer_size=buffer_capacity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "unauthorized-style",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47: episode 1 done, reward=46.000, epsilon=0.86\n",
      "65: episode 2 done, reward=18.000, epsilon=0.72\n",
      "97: episode 3 done, reward=32.000, epsilon=0.52\n",
      "116: episode 4 done, reward=19.000, epsilon=0.43\n",
      "127: episode 5 done, reward=11.000, epsilon=0.38\n",
      "139: episode 6 done, reward=12.000, epsilon=0.34\n",
      "155: episode 7 done, reward=16.000, epsilon=0.29\n",
      "166: episode 8 done, reward=11.000, epsilon=0.26\n",
      "177: episode 9 done, reward=11.000, epsilon=0.23\n",
      "186: episode 10 done, reward=9.000, epsilon=0.21\n",
      "195: episode 11 done, reward=9.000, epsilon=0.19\n",
      "204: episode 12 done, reward=9.000, epsilon=0.18\n",
      "216: episode 13 done, reward=12.000, epsilon=0.16\n",
      "225: episode 14 done, reward=9.000, epsilon=0.14\n",
      "238: episode 15 done, reward=13.000, epsilon=0.13\n",
      "248: episode 16 done, reward=10.000, epsilon=0.11\n",
      "258: episode 17 done, reward=10.000, epsilon=0.10\n",
      "268: episode 18 done, reward=10.000, epsilon=0.09\n",
      "278: episode 19 done, reward=10.000, epsilon=0.08\n",
      "286: episode 20 done, reward=8.000, epsilon=0.08\n",
      "296: episode 21 done, reward=10.000, epsilon=0.07\n",
      "306: episode 22 done, reward=10.000, epsilon=0.06\n",
      "319: episode 23 done, reward=13.000, epsilon=0.06\n",
      "336: episode 24 done, reward=17.000, epsilon=0.05\n",
      "363: episode 25 done, reward=27.000, epsilon=0.04\n",
      "373: episode 26 done, reward=10.000, epsilon=0.03\n",
      "383: episode 27 done, reward=10.000, epsilon=0.03\n",
      "393: episode 28 done, reward=10.000, epsilon=0.03\n",
      "405: episode 29 done, reward=12.000, epsilon=0.02\n",
      "421: episode 30 done, reward=16.000, epsilon=0.02\n",
      "437: episode 31 done, reward=16.000, epsilon=0.02\n",
      "515: episode 32 done, reward=78.000, epsilon=0.01\n",
      "574: episode 33 done, reward=59.000, epsilon=0.00\n",
      "653: episode 34 done, reward=79.000, epsilon=0.00\n",
      "680: episode 35 done, reward=27.000, epsilon=0.00\n",
      "695: episode 36 done, reward=15.000, epsilon=0.00\n",
      "711: episode 37 done, reward=16.000, epsilon=0.00\n",
      "752: episode 38 done, reward=41.000, epsilon=0.00\n",
      "768: episode 39 done, reward=16.000, epsilon=0.00\n",
      "963: episode 40 done, reward=195.000, epsilon=0.00\n",
      "Solved\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "episode = 0\n",
    "reward = 0.0\n",
    "solved = False\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Sample new experiences from the environment into the ER buffer\n",
    "    step += 1\n",
    "    replay_buffer.populate(1)\n",
    "\n",
    "    # Look for a completed episode since the last call\n",
    "    for reward, steps in exp_source.pop_rewards_steps():\n",
    "        episode += 1\n",
    "        print(\n",
    "            f\"{step}: episode {episode} done, reward={reward:.3f}, \"\n",
    "            f\"epsilon={selector.epsilon:.2f}\"\n",
    "        )\n",
    "        solved = reward > solution_reward_bound\n",
    "\n",
    "    # Termination condition\n",
    "    if solved:\n",
    "        print(\"Solved\")\n",
    "        break\n",
    "\n",
    "    # Let the ER buffer fill up\n",
    "    if len(replay_buffer) < min_buffer_size:\n",
    "        continue\n",
    "\n",
    "    # Sample a batch from the ER buffer\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Extract all the states, actions and TD targets from the batch\n",
    "    #  - Note: TD targets are computed using the TargetNet, hence fixed\n",
    "    states, actions, td_targets = unpack_batch(\n",
    "        batch=batch,\n",
    "        net=target_net.target_model,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "\n",
    "    # Reset gradients before the next backpropagation step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Let the main DQN compute Q values of the actions played in the batch\n",
    "    q_values = main_net(states)\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Compute the gradient of the MSE between Q values end TD targets\n",
    "    loss = torch.nn.functional.mse_loss(q_values, td_targets)\n",
    "    loss.backward()\n",
    "\n",
    "    # Make single gradient descent step and update weights of the main DQN\n",
    "    optimizer.step()\n",
    "\n",
    "    # Decay epsilon based on a fixed rate schedule\n",
    "    selector.epsilon *= epsilon_decay\n",
    "\n",
    "    # Periodically synchronize weights of the main and target DQNs\n",
    "    if step % target_sync_period == 0:\n",
    "        target_net.sync()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
