{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specialized-format",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-perry",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "*Tensors* represent\n",
    "* *multilinear maps* between vector spaces (mathematics)\n",
    "* generic *n-dimensional arrays* (computer science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expensive-acrylic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3556e-19, 3.0097e+29],\n",
       "        [7.1853e+22, 4.5145e+27],\n",
       "        [1.8040e+28, 1.5769e-19]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create an uninitialized 3x2 tensor of 32-bit floats\n",
    "a = torch.FloatTensor(3, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "billion-resident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tensor (in-place) with zeros\n",
    "a.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-diagram",
   "metadata": {},
   "source": [
    "There are two types of methods in the PyTorch API:\n",
    "* Functional ones that return transformed copies have standard names like `some_function()`\n",
    "* In-place mutating operations will have a trailing underscore in their name, e.g. `some_function_()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complete-pipeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [3., 2., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor from a standard collection\n",
    "torch.FloatTensor([[1, 2, 3], [3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gentle-blackjack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.zeros(shape=(3, 2))\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "organizational-variety",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor from a numpy ndarray\n",
    "b = torch.tensor(n)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "level-chile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the numpy array to 64-bit float\n",
    "#  - This translates to the tensor\n",
    "n = np.zeros(shape=(3, 2), dtype=np.float32)\n",
    "torch.tensor(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "embedded-feeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively specify a PyTorch dtype\n",
    "n = np.zeros(shape=(3, 2))\n",
    "torch.tensor(n, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "voluntary-bicycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "southwest-moment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar tensors can be results of some aggregations\n",
    "s = a.sum()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "centered-concrete",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There's a convenient method to access the value of a scalar tensor\n",
    "s.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "small-database",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-belize",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "Each tensor has associated `device` where the computation takes place. The options are\n",
    "* `cpu` - computation takes place on the CPU\n",
    "* `cuda` or `cuda:<index>` - computation takes place on the GPU (with a device id `<index>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "progressive-religious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine computation device based on CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "a = torch.FloatTensor([2, 3])\n",
    "\n",
    "# Move the tensor to GPU (if there's CUDA available)\n",
    "a = a.to(device)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "regional-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-disclosure",
   "metadata": {},
   "source": [
    "### Tensors and Gradients\n",
    "Each tensor has following info related to automatic gradient computation:\n",
    "* `grad` is a property holding computed gradients (tensor of the same shape)\n",
    "* `is_leaf` is true if the tensor was constructed by a user and false if it's a result of a computation\n",
    "* `requires_grad` is true if the tensor requires gradients to be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "occasional-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some tensors\n",
    "#  - The first one requires gradients to be computed\n",
    "v1 = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "v2 = torch.tensor([2.0, 2.0])\n",
    "\n",
    "# Define a computational graph on these tensors\n",
    "#  - Notice: Result contains a function coputing the gradient.\n",
    "v_sum = v1 + v2\n",
    "v_res = (v_sum * 2).sum()\n",
    "v_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "extraordinary-survival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.is_leaf, v2.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hourly-airfare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sum.is_leaf, v_res.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dominican-myrtle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.requires_grad, v2.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "upper-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sum.requires_grad, v_res.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "modern-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the gradients of our graph\n",
    "v_res.backward()\n",
    "\n",
    "# Show backpropagated gradients in v1\n",
    "v1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bacterial-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 does not require any gradients so there's nothing\n",
    "v2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-description",
   "metadata": {},
   "source": [
    "## Neural Network Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interested-record",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1789, -0.4192, -0.8343, -1.2979, -0.6996], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn  # noqa\n",
    "\n",
    "# Construct a 2-to-5 dense layer with an implicit bias\n",
    "#  - Note: Weights of this layer are randomly initialized.\n",
    "dense = nn.Linear(2, 5)\n",
    "\n",
    "# Each PyTorch NN module acts as a callable\n",
    "inputs = torch.FloatTensor([1, 2])\n",
    "dense(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-marijuana",
   "metadata": {},
   "source": [
    "Some important methods from PyTorch API:\n",
    "* `parameters()` returns an iterable of all trainable variables (those that require gradients)\n",
    "* `zero_grad()` initializes all gradients to zero\n",
    "* `to(device)` moves the computation to a device\n",
    "* `state_dict()` exports all weights to a dictionary for model serialization\n",
    "* `load_state_dict()` oppisite of the previous which imports weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "solar-medline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=20, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Dropout(p=0.3, inplace=False)\n",
       "  (7): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a sequential model with\n",
    "#  - Three dense layers with ReLU activations\n",
    "#  - A dropout layer\n",
    "#  - And a softmax output over the feature dimension\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hearing-duncan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0937, 0.1025, 0.0937, 0.1092, 0.0937, 0.0937, 0.1323, 0.0937, 0.0937,\n",
       "         0.0937]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed an input tensor through our sequential model\n",
    "#  - There's single instance in the input batch\n",
    "model(torch.FloatTensor([[1, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-juice",
   "metadata": {},
   "source": [
    "### Custom Layers\n",
    "Creating custom modules (layers) is as easy as inheriting from `nn.Module` class and implementing the `forward()` method. Every other instance of a module assigned to a field is automatically registered under this module.\n",
    "\n",
    "Note that the convention is to use the module as a callable - this is because the `Module` class does some extra work in the `__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "leading-september",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModule(\n",
       "  (pipe): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypeVar  # noqa\n",
    "\n",
    "T = TypeVar(\"T\", bound=torch.Tensor)\n",
    "\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    \"\"\"Custom PyTorch module\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_inputs: int,\n",
    "        n_outputs: int,\n",
    "        dropout_prob: float = 0.3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Build a sequential model\n",
    "        #  - Every field that is a Module is auto-discovered\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, n_outputs),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T) -> T:\n",
    "        # We must treat the sub-module as a callable!\n",
    "        return self.pipe(x)\n",
    "\n",
    "\n",
    "# Build an instance of this model and show its structure\n",
    "net = MyModule(n_inputs=2, n_outputs=3)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "reserved-gates",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3437, 0.2639, 0.3924]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed an input batch to the model\n",
    "net(torch.FloatTensor([[2, 3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-accordance",
   "metadata": {},
   "source": [
    "### Loss Functions and Optimizers\n",
    "\n",
    "PyTorch includes standard set of loss functions and of course allows simple implementation of custom ones. Here's a short list of some loss function classes:\n",
    "* `nn.MSELoss` is the *mean squared error* typically used for regression problems\n",
    "* `nn.BCELoss` and `BCEWithLogits` are *binary cross-entropy* losses for binary classification problems - the former expects single probability value while the latter raw scores (usually preferable)\n",
    "* `nn.CrossEntropyLoss` and `nn.NLLLoss` for multi-class classification problems\n",
    "\n",
    "Similarly there is buch of traditional optimizers such as vanilla `SGD`, `RMSprop`, `Adagrad` or the popular `Adam`. Finally, here's a typical training loop in PyTorch.\n",
    "```python\n",
    "# Define model and loss function\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "\n",
    "# Register all trainable parameters in an optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), ...)\n",
    "\n",
    "# Iterate over mini-batches of training data\n",
    "for X_train, y_trian in iterate_batches(data, batch_size=32)\n",
    "    \n",
    "    # Wrap examples and labels into tensors\n",
    "    X_train = torch.tensor(X_train)\n",
    "    y_trian = torch.tensor(y_trian)\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Compute model's prediction loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Compute gradients of the loss function w.r.t. all the weights\n",
    "    #  - The loss is just a computation graph over tensors in the model\n",
    "    #  - And because model's weights \"require gradients\" this calculates dL/dw\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform one gradient descent step using computed gradients\n",
    "    #  - Note: The optimizer has access to `grad` for registered `params`\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Clear gradients for this step\n",
    "    #  - Alternatively this can be done as the beginning of a step\n",
    "    #  - Note: This is a convenience method for calling it on the model\n",
    "    optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-dispute",
   "metadata": {},
   "source": [
    "## Monitoring with TensorBoard(X)\n",
    "Following example shows how to log arbitrary metrics and investigate them with *TensorBoard*. Because TensorBoard expects data in *TensorFlow* format we use `tensorboardX` for easy integration (also because it's a dependecy of PyTorch Ignite that we'll use later).\n",
    "\n",
    "Example below computes values of few trigonometric functions for varying angles and stores the output in `runs/` director. For later view one can run TensorBoard with\n",
    "```bash\n",
    "tensorboard --log-dir runs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "moved-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # noqa\n",
    "\n",
    "from tensorboardX import SummaryWriter  # noqa\n",
    "\n",
    "# Define some functions representing our metrics\n",
    "funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n",
    "\n",
    "# Create tesorboardX writer\n",
    "#  - Note: The default output directory is './runs'\n",
    "#  - Note 2: By default each call creates new \"run\"\n",
    "with SummaryWriter() as writer:\n",
    "\n",
    "    # Register one metric per function\n",
    "    for name, f in funcs.items():\n",
    "\n",
    "        # Evaluate and record f on interval [-360, 360)\n",
    "        for angle in range(-360, 360):\n",
    "            val = f(angle * math.pi / 180)\n",
    "            writer.add_scalar(name, val, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "korean-track",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mruns\u001b[00m\n",
      "├── \u001b[01;34mMar10_15-13-12_mpc-xps\u001b[00m\n",
      "│   └── events.out.tfevents.1615385592.mpc-xps\n",
      "└── \u001b[01;34mMar10_15-14-45_mpc-xps\u001b[00m\n",
      "    └── events.out.tfevents.1615385685.mpc-xps\n",
      "\n",
      "2 directories, 2 files\n"
     ]
    }
   ],
   "source": [
    "!tree runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-clark",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
