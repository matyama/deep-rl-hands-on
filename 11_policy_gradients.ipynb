{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pregnant-implementation",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/matyama/deep-rl-hands-on/blob/main/11_policy_gradients.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "        Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "outstanding-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "\n",
    "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
    "pip install ptan>=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-creativity",
   "metadata": {},
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assigned-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa: E402,I001\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ptan\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ptan.experience import ExperienceFirstLast\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-desperate",
   "metadata": {},
   "source": [
    "## Values and Policy\n",
    "Contrary to the value iteration methods (Q-Learning) which try to estimate the state values (state-action values), the *policy gradient* technique focus directly on the policy $\\pi(s)$. \n",
    "\n",
    "Direct policy modeling has several advantages:\n",
    "* From certain point of view, we don't care that much about the expected discounted rewards but rather the decision/action $\\pi(s)$ to take in each state $s$\n",
    "* As we saw earlier with the *Categorical DQN*, learning a distribution helps to better capture the underlying MDP (especially in stochastic environments)\n",
    "* It becomes quite a hard to determine the best action to take when the action space is large or even continuous. The DQN model of $Q(s, a)$ is highly non-linear and the optimization problem $a^* = argmax_a Q(s, a)$ can be hard to solve.\n",
    "\n",
    "In the value iteration case our DQN parametrized the state-action values as $DQN(s) \\to Q_\\mathbf{w}(s, \\cdot)$. Similarly, we will represent the policy as a probability distribution over actions $\\pi_\\mathbf{w}(s)$ parametrized by the NN.\n",
    "\n",
    "*Modelling the output as action (class) probabilities is a typical technique in classification tasks that gives us a smooth representation (intuitively, changing NN weights $\\mathbf{w}$ a bit changes $\\pi$ a bit as well - compared to the case with discrete action labels which would change in steps).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infectious-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Gradient Network that consumes states (observations)\n",
    "    and outputs action logits (scores).\n",
    "\n",
    "    Note: Logits should be manually converted to probabilities with\n",
    "    `log_softmax` for better numerical stability and optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: Tuple[int, ...], n_actions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Simple, not really deep, forward network that outputs action logits\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-exhaust",
   "metadata": {},
   "source": [
    "## Gradients of the Policy\n",
    "\n",
    "*Policy Gradient* methods are closely related to the *Cross-Entropy Method* introduced earlier. The gradient is a direction in which we want to change NN weights to maximize the accumulated reward and is proportional in scale to the $Q$ state-action value and in the direction to the log of action probabilities:\n",
    "$$\n",
    "\\nabla J \\approx \\mathbb{E}[Q(s, a) \\nabla \\log(\\pi(a | s))]\n",
    "$$\n",
    "where the expectation means that we average the gradient over several steps.\n",
    "\n",
    "Equivalently we can say that we optimize the loss function $\\mathcal{L} = -Q(s, a) \\log(\\pi(a | s))$ (Note: SGD minimizes the loss function but we want to maximize the gradient, therefore the minus sign).\n",
    "\n",
    "Recall that in the *Cross-Entropy Method* we sampled the environment for few episodes and trained only on transitions from the above-average ones. This corresponds to having $Q(s, a) = 1$ for the good transitions and $Q(s, a) = 0$ otherwise. In general, policy gradient methods differ in the way how $Q$ values are treated but in any case we want to use $Q(s, a) \\in [0, 1]$:\n",
    "1. for better separation of episode\n",
    "1. to incorporate the discount factor and thus the uncertainty about future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-unemployment",
   "metadata": {},
   "source": [
    "## The REINFORCE method\n",
    "The outline of the *REINFORCE* methods is the following:\n",
    "1. Initialize NN weights randomly\n",
    "1. Play $N$ full episode and collect experiences $(s, a, r, s')$\n",
    "1. Compute actual $Q$ values for every played episode $k$ and step $t$: $Q_{k, t} = \\sum_{i=0}^t \\gamma^t r_t$\n",
    "1. Compute the loss for all transitions: $\\mathcal{L} = - \\sum_{k, t} Q_{k, t} \\log(\\pi(s_{k, t}, a_{k, t}))$\n",
    "1. Do one SGD step by minimizing the loss and update NN weights\n",
    "1. Repeat from step 2. until convergence\n",
    "\n",
    "Properties of the REINFORCE method:\n",
    "* We **don't need an explicit exploration policy** because we explore automatically using the policy our NN outputs.\n",
    "* **On-policy** method, therefore no ER buffer is needed because we can't train on the data from old policies. On the other hand, value methods typically need less interations with the environment.\n",
    "* We train on actual Q values and not estimated ones so we **don't need a target NN** to break experience correlations either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-jimmy",
   "metadata": {},
   "source": [
    "### CartPole REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blank-reunion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matyama/.cache/pypoetry/virtualenvs/deep-rl-hands-on-A12lkhIw-py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411: reward: 7.9e+01, mean_100: 4.1e+01, episodes: 10\n",
      "806: reward: 5.1e+01, mean_100: 4e+01, episodes: 20\n",
      "1116: reward: 1.6e+01, mean_100: 3.7e+01, episodes: 30\n",
      "1621: reward: 6.1e+01, mean_100: 4.1e+01, episodes: 40\n",
      "2150: reward: 4.5e+01, mean_100: 4.3e+01, episodes: 50\n",
      "2586: reward: 3.5e+01, mean_100: 4.3e+01, episodes: 60\n",
      "3048: reward: 7.4e+01, mean_100: 4.4e+01, episodes: 70\n",
      "3657: reward: 7.2e+01, mean_100: 4.6e+01, episodes: 80\n",
      "4552: reward: 4.9e+01, mean_100: 5.1e+01, episodes: 90\n",
      "5430: reward: 6e+01, mean_100: 5.4e+01, episodes: 100\n",
      "6305: reward: 2.3e+01, mean_100: 5.9e+01, episodes: 110\n",
      "7668: reward: 2e+02, mean_100: 6.9e+01, episodes: 120\n",
      "9500: reward: 2e+02, mean_100: 8.4e+01, episodes: 130\n",
      "11123: reward: 1.9e+02, mean_100: 9.5e+01, episodes: 140\n",
      "12196: reward: 1e+02, mean_100: 1e+02, episodes: 150\n",
      "12973: reward: 9.6e+01, mean_100: 1e+02, episodes: 160\n",
      "13592: reward: 5.4e+01, mean_100: 1.1e+02, episodes: 170\n",
      "14663: reward: 1.2e+02, mean_100: 1.1e+02, episodes: 180\n",
      "16031: reward: 1.4e+02, mean_100: 1.1e+02, episodes: 190\n",
      "18017: reward: 2e+02, mean_100: 1.3e+02, episodes: 200\n",
      "20017: reward: 2e+02, mean_100: 1.4e+02, episodes: 210\n",
      "21800: reward: 1.3e+02, mean_100: 1.4e+02, episodes: 220\n",
      "23511: reward: 2e+02, mean_100: 1.4e+02, episodes: 230\n",
      "25410: reward: 2e+02, mean_100: 1.4e+02, episodes: 240\n",
      "27160: reward: 2e+02, mean_100: 1.5e+02, episodes: 250\n",
      "29160: reward: 2e+02, mean_100: 1.6e+02, episodes: 260\n",
      "30953: reward: 2e+02, mean_100: 1.7e+02, episodes: 270\n",
      "32826: reward: 2e+02, mean_100: 1.8e+02, episodes: 280\n",
      "34638: reward: 2e+02, mean_100: 1.9e+02, episodes: 290\n",
      "36573: reward: 2e+02, mean_100: 1.9e+02, episodes: 300\n",
      "38566: reward: 2e+02, mean_100: 1.9e+02, episodes: 310\n",
      "40450: reward: 1.7e+02, mean_100: 1.9e+02, episodes: 320\n",
      "42145: reward: 2e+02, mean_100: 1.9e+02, episodes: 330\n",
      "43864: reward: 2e+02, mean_100: 1.8e+02, episodes: 340\n",
      "44951: reward: 4.4e+01, mean_100: 1.8e+02, episodes: 350\n",
      "45539: reward: 3.7e+01, mean_100: 1.6e+02, episodes: 360\n",
      "45917: reward: 3.2e+01, mean_100: 1.5e+02, episodes: 370\n",
      "46262: reward: 3.2e+01, mean_100: 1.3e+02, episodes: 380\n",
      "46594: reward: 4e+01, mean_100: 1.2e+02, episodes: 390\n",
      "46914: reward: 2.7e+01, mean_100: 1e+02, episodes: 400\n",
      "47200: reward: 2.9e+01, mean_100: 8.6e+01, episodes: 410\n",
      "47528: reward: 1.7e+01, mean_100: 7.1e+01, episodes: 420\n",
      "47880: reward: 3.1e+01, mean_100: 5.7e+01, episodes: 430\n",
      "48250: reward: 3.9e+01, mean_100: 4.4e+01, episodes: 440\n",
      "48681: reward: 4.2e+01, mean_100: 3.7e+01, episodes: 450\n",
      "49179: reward: 8.3e+01, mean_100: 3.6e+01, episodes: 460\n",
      "49861: reward: 9.5e+01, mean_100: 3.9e+01, episodes: 470\n",
      "50889: reward: 1.2e+02, mean_100: 4.6e+01, episodes: 480\n",
      "52127: reward: 1.2e+02, mean_100: 5.5e+01, episodes: 490\n",
      "53562: reward: 1.7e+02, mean_100: 6.6e+01, episodes: 500\n",
      "55145: reward: 1.6e+02, mean_100: 7.9e+01, episodes: 510\n",
      "56819: reward: 1.7e+02, mean_100: 9.3e+01, episodes: 520\n",
      "58430: reward: 1.5e+02, mean_100: 1.1e+02, episodes: 530\n",
      "60205: reward: 1.8e+02, mean_100: 1.2e+02, episodes: 540\n",
      "62147: reward: 2e+02, mean_100: 1.3e+02, episodes: 550\n",
      "64147: reward: 2e+02, mean_100: 1.5e+02, episodes: 560\n",
      "66147: reward: 2e+02, mean_100: 1.6e+02, episodes: 570\n",
      "68147: reward: 2e+02, mean_100: 1.7e+02, episodes: 580\n",
      "70147: reward: 2e+02, mean_100: 1.8e+02, episodes: 590\n",
      "72147: reward: 2e+02, mean_100: 1.9e+02, episodes: 600\n",
      "74147: reward: 2e+02, mean_100: 1.9e+02, episodes: 610\n",
      "75963: reward: 2e+02, mean_100: 1.9e+02, episodes: 620\n",
      "77963: reward: 2e+02, mean_100: 2e+02, episodes: 630\n",
      "Solved in 77963 steps and 630 episodes!\n"
     ]
    }
   ],
   "source": [
    "def compute_q_values(rewards: List[float], gamma: float) -> Iterable[float]:\n",
    "    qs = []\n",
    "    sum_r = 0.0\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= gamma\n",
    "        sum_r += r\n",
    "        qs.append(sum_r)\n",
    "\n",
    "    return reversed(qs)\n",
    "\n",
    "\n",
    "def train_reinforce(\n",
    "    env_name: str,\n",
    "    gamma: float = 0.99,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_played_episodes: int = 4,\n",
    "    reward_bound: int = 195,\n",
    "    log_period: int = 10,\n",
    ") -> None:\n",
    "\n",
    "    # Crate the environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Create PG network\n",
    "    net = PGN(\n",
    "        input_shape=env.observation_space.shape[0],\n",
    "        n_actions=env.action_space.n,\n",
    "    )\n",
    "    print(net)\n",
    "\n",
    "    # Initialize an agent\n",
    "    #  - Notice: We instruct it to apply softmax to the PGN output\n",
    "    agent = ptan.agent.PolicyAgent(\n",
    "        net,\n",
    "        preprocessor=ptan.agent.float32_preprocessor,\n",
    "        apply_softmax=True,\n",
    "    )\n",
    "\n",
    "    # Create experience source and optimizer\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    with SummaryWriter(comment=f\"-{env_name}-reinforce\") as writer:\n",
    "\n",
    "        done_episodes = 0\n",
    "        batch_episodes = 0\n",
    "\n",
    "        batch_states, batch_actions, batch_q_values = [], [], []\n",
    "\n",
    "        episode_rewards = []\n",
    "        total_rewards = []\n",
    "\n",
    "        # Interact with the environment and consume experiences\n",
    "        for i, exp in enumerate(exp_source):\n",
    "\n",
    "            # Add the new experience to current batch\n",
    "            batch_states.append(exp.state)\n",
    "            batch_actions.append(int(exp.action))\n",
    "\n",
    "            # Buffer immedieate rewards during each episode\n",
    "            episode_rewards.append(exp.reward)\n",
    "\n",
    "            # Compute Q values from immediate rewards when episode ends\n",
    "            if exp.last_state is None:\n",
    "                batch_q_values += compute_q_values(episode_rewards, gamma)\n",
    "                episode_rewards.clear()\n",
    "                batch_episodes += 1\n",
    "\n",
    "            # Handle new rewards\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "\n",
    "                done_episodes += 1\n",
    "\n",
    "                # Collect total rewards\n",
    "                reward = new_rewards[0]\n",
    "                total_rewards.append(reward)\n",
    "\n",
    "                # Compute the mean reward over last 100 episodes\n",
    "                mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "\n",
    "                # Log training progress\n",
    "                if done_episodes % log_period == 0:\n",
    "                    print(\n",
    "                        f\"{i}: reward: {reward:.2}, \"\n",
    "                        f\"mean_100: {mean_rewards:.2}, \"\n",
    "                        f\"episodes: {done_episodes}\"\n",
    "                    )\n",
    "\n",
    "                # Record metrics for TensorBoard\n",
    "                writer.add_scalar(\"reward\", reward, i)\n",
    "                writer.add_scalar(\"reward_100\", mean_rewards, i)\n",
    "                writer.add_scalar(\"episodes\", done_episodes, i)\n",
    "\n",
    "                # Check if the learned policy is good enough\n",
    "                if mean_rewards > reward_bound:\n",
    "                    print(f\"Solved in {i} steps and {done_episodes} episodes!\")\n",
    "                    break\n",
    "\n",
    "            # Play N episodes to accumulate Q values before training step\n",
    "            if batch_episodes < n_played_episodes:\n",
    "                continue\n",
    "\n",
    "            n_states = len(batch_states)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Convert batch parts to tensors\n",
    "            states = torch.FloatTensor(batch_states)\n",
    "            actions = torch.LongTensor(batch_actions)\n",
    "            q_values = torch.FloatTensor(batch_q_values)\n",
    "\n",
    "            # Compute action scores (logits)\n",
    "            #  - Note: There's just single pass through the PGN (DQN has 2)\n",
    "            logits = net(states)\n",
    "\n",
    "            # Compute the loss funciton defiend in previous section\n",
    "            log_action_prob = nn.functional.log_softmax(logits, dim=1)\n",
    "            exp_values = q_values * log_action_prob[range(n_states), actions]\n",
    "            loss = -exp_values.mean()\n",
    "\n",
    "            # Compute gradient of the loss function and make one SGD step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reset current batch\n",
    "            batch_episodes = 0\n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_q_values.clear()\n",
    "\n",
    "\n",
    "# Run REINFORCE to solve the CartPole environment\n",
    "train_reinforce(env_name=\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-stanford",
   "metadata": {},
   "source": [
    "### REINFORCE issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-warning",
   "metadata": {},
   "source": [
    "#### Complete episodes\n",
    "First drawback of REINFORCE and PG methods in general is that it is way **less sample efficient**. In order to estimate Q values as well as possible we need quite a lot of interations with the environment from full episodes. Moreover, the length of the episodes we must play only inreases in complex environments (e.g. episodes in Atari Pong might have thousands of steps).\n",
    "\n",
    "In the DQN scenario we used our own $Q(s, a)$ to estimate $V(s)$ in the one-step Bellman update: $Q(s, a) = r_a + \\gamma V(s')$. But in PG we don't have Q values - these are approximated from episodes completed in the environment.\n",
    "\n",
    "There are two approaches dealing with this issue:\n",
    "* We use the NN to estimate $V(s)$ as well as action logits and use these state values to obtain Qs. This approach implements the *actor-critic method* which will be described later.\n",
    "* The other way is to unroll the Bellman Eq. N steps ahead which will implicitly exploit the fact that the future value contribution is discounted by $\\gamma < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-horse",
   "metadata": {},
   "source": [
    "#### High variance of gradients\n",
    "Recall that the policy gradient $\\nabla J$ is proportinal to $Q(s, a)$. The problem with rewards (and thus Q values) is that these are heavily environment-dependent. In other words, the gradient has high variance - one lucky episode will dominate in the final gradient.\n",
    "\n",
    "To prevent training instabilities due to high variance one can subtract a *baseline* value from the $Q$:\n",
    "* Constant value, typically the mean of the discounted rewards\n",
    "* Moving average of the discounted rewards\n",
    "* The state value $V(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-monitoring",
   "metadata": {},
   "source": [
    "#### Exploration\n",
    "Even though we can get rid of exploration strategies (e.g. epsilon-greedy) because we can sample from current policy, the agent can still converge to a sub-optimal policy. Fortunately, we can benefit from the fact that we have represented the policy as a probability distribution and add an *entropy bonus* to the loss funciton.\n",
    "\n",
    "The entropy of a policy is\n",
    "$$\n",
    "H(\\pi(\\cdot | s)) = - \\sum_a \\pi(a | s) \\log(\\pi(a | s))\n",
    "$$\n",
    "and we add it to the loss (or rather the mean over batch states $s$) in order to push the agent from local optima by promoting more uniform distribution over actions (in local optima some action $a$ will have $\\pi(a | \\cdot) = 1$ which corresponds to $H(\\pi) = 0$, here we constrain it by maximizing over $H$ as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-hobby",
   "metadata": {},
   "source": [
    "#### Correlation between samples\n",
    "As mentioned before, we cannot use an experience replay buffer as we did in DQN to break correlations between experiences from one episode because PG is an *on-policy* method. If we did use old experiences, we'd compute gradient of an old policy, not the current one.\n",
    "\n",
    "A typical trick to solve this problem for the PG methods is to sample from multiple environments (independent but the same) at once. This will give us an i.i.d. set of experiences for the SGD step (or close enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-stocks",
   "metadata": {},
   "source": [
    "## Policy Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-phase",
   "metadata": {},
   "source": [
    "### CartPole PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "organized-liberal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "193: reward: 3.7e+01, mean_100: 1.9e+01, episodes: 10\n",
      "394: reward: 2e+01, mean_100: 2e+01, episodes: 20\n",
      "619: reward: 3.2e+01, mean_100: 2.1e+01, episodes: 30\n",
      "848: reward: 1.4e+01, mean_100: 2.1e+01, episodes: 40\n",
      "1163: reward: 1.4e+01, mean_100: 2.3e+01, episodes: 50\n",
      "1461: reward: 3e+01, mean_100: 2.4e+01, episodes: 60\n",
      "1737: reward: 2.5e+01, mean_100: 2.5e+01, episodes: 70\n",
      "2221: reward: 1.7e+01, mean_100: 2.8e+01, episodes: 80\n",
      "2753: reward: 2.5e+01, mean_100: 3.1e+01, episodes: 90\n",
      "3345: reward: 5.2e+01, mean_100: 3.3e+01, episodes: 100\n",
      "3984: reward: 1.3e+02, mean_100: 3.8e+01, episodes: 110\n",
      "4680: reward: 4.7e+01, mean_100: 4.3e+01, episodes: 120\n",
      "5787: reward: 1.6e+02, mean_100: 5.2e+01, episodes: 130\n",
      "7117: reward: 1.6e+02, mean_100: 6.3e+01, episodes: 140\n",
      "8416: reward: 1.1e+02, mean_100: 7.3e+01, episodes: 150\n",
      "9527: reward: 1.6e+02, mean_100: 8.1e+01, episodes: 160\n",
      "11129: reward: 1.2e+02, mean_100: 9.4e+01, episodes: 170\n",
      "12719: reward: 2e+02, mean_100: 1e+02, episodes: 180\n",
      "14068: reward: 2e+02, mean_100: 1.1e+02, episodes: 190\n",
      "15756: reward: 2.3e+01, mean_100: 1.2e+02, episodes: 200\n",
      "17655: reward: 1.8e+02, mean_100: 1.4e+02, episodes: 210\n",
      "19247: reward: 2e+02, mean_100: 1.5e+02, episodes: 220\n",
      "21029: reward: 2e+02, mean_100: 1.5e+02, episodes: 230\n",
      "22977: reward: 1.9e+02, mean_100: 1.6e+02, episodes: 240\n",
      "24681: reward: 2e+02, mean_100: 1.6e+02, episodes: 250\n",
      "26681: reward: 2e+02, mean_100: 1.7e+02, episodes: 260\n",
      "28681: reward: 2e+02, mean_100: 1.8e+02, episodes: 270\n",
      "30681: reward: 2e+02, mean_100: 1.8e+02, episodes: 280\n",
      "32681: reward: 2e+02, mean_100: 1.9e+02, episodes: 290\n",
      "34540: reward: 2e+02, mean_100: 1.9e+02, episodes: 300\n",
      "36540: reward: 2e+02, mean_100: 1.9e+02, episodes: 310\n",
      "38418: reward: 2e+02, mean_100: 1.9e+02, episodes: 320\n",
      "40297: reward: 2e+02, mean_100: 1.9e+02, episodes: 330\n",
      "42112: reward: 1.9e+02, mean_100: 1.9e+02, episodes: 340\n",
      "43866: reward: 2e+02, mean_100: 1.9e+02, episodes: 350\n",
      "45513: reward: 2e+02, mean_100: 1.9e+02, episodes: 360\n",
      "47513: reward: 2e+02, mean_100: 1.9e+02, episodes: 370\n",
      "49290: reward: 2e+02, mean_100: 1.9e+02, episodes: 380\n",
      "51290: reward: 2e+02, mean_100: 1.9e+02, episodes: 390\n",
      "53201: reward: 2e+02, mean_100: 1.9e+02, episodes: 400\n",
      "55201: reward: 2e+02, mean_100: 1.9e+02, episodes: 410\n",
      "57059: reward: 2e+02, mean_100: 1.9e+02, episodes: 420\n",
      "59059: reward: 2e+02, mean_100: 1.9e+02, episodes: 430\n",
      "61035: reward: 2e+02, mean_100: 1.9e+02, episodes: 440\n",
      "62886: reward: 2e+02, mean_100: 1.9e+02, episodes: 450\n",
      "64558: reward: 2e+01, mean_100: 1.9e+02, episodes: 460\n",
      "66558: reward: 2e+02, mean_100: 1.9e+02, episodes: 470\n",
      "68558: reward: 2e+02, mean_100: 1.9e+02, episodes: 480\n",
      "70558: reward: 2e+02, mean_100: 1.9e+02, episodes: 490\n",
      "72558: reward: 2e+02, mean_100: 1.9e+02, episodes: 500\n",
      "74558: reward: 2e+02, mean_100: 1.9e+02, episodes: 510\n",
      "76558: reward: 2e+02, mean_100: 1.9e+02, episodes: 520\n",
      "78558: reward: 2e+02, mean_100: 1.9e+02, episodes: 530\n",
      "Solved in 79158 steps and 533 episodes!\n"
     ]
    }
   ],
   "source": [
    "def smooth(old: Optional[float], val: float, alpha: float = 0.95) -> float:\n",
    "    return val if old is None else old * alpha + (1 - alpha) * val\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "REWARD_STEPS = 10\n",
    "REWARD_BOUND = 195\n",
    "LOG_PERIOD = 10\n",
    "\n",
    "# Initialize entironment, PGN, the agent, exp. source and optimizer\n",
    "#  - We pass gamma directly to the exp. source to discount the rewards\n",
    "#  - We also use `REWARD_STEPS`-ahead technique with our exp. source\n",
    "#    instead of playing full episodes to approximate Q values\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "net = PGN(\n",
    "    input_shape=env.observation_space.shape[0],\n",
    "    n_actions=env.action_space.n,\n",
    ")\n",
    "print(net)\n",
    "\n",
    "agent = ptan.agent.PolicyAgent(\n",
    "    net,\n",
    "    preprocessor=ptan.agent.float32_preprocessor,\n",
    "    apply_softmax=True,\n",
    ")\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    gamma=GAMMA,\n",
    "    steps_count=REWARD_STEPS,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "with SummaryWriter(comment=\"-cartpole-pg\") as writer:\n",
    "\n",
    "    done_episodes = 0\n",
    "\n",
    "    bs_smoothed = entropy = l_entropy = l_policy = l_total = None\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "\n",
    "    total_rewards = []\n",
    "    step_rewards = []\n",
    "    reward_sum = 0.0\n",
    "\n",
    "    # Run the training loop\n",
    "    for i, exp in enumerate(exp_source):\n",
    "\n",
    "        # Accumulate discounted rewards over `REWARD_STEPS`-ahead\n",
    "        reward_sum += exp.reward\n",
    "\n",
    "        # Compute current baseline value as the mean reward up until now\n",
    "        baseline = reward_sum / (i + 1)\n",
    "\n",
    "        # Track the baseline value\n",
    "        writer.add_scalar(\"baseline\", baseline, i)\n",
    "\n",
    "        # Add new experience to the batch\n",
    "        #  - Notice: We subtract the baseline value from the reward to\n",
    "        #    reduce the variance of the gradient scales (Q values)\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "        # Handle new rewards as before\n",
    "        #  - Logs training progress and metrics in previous example\n",
    "        #  - The termination condition is also the same\n",
    "\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "\n",
    "            done_episodes += 1\n",
    "\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "\n",
    "            if done_episodes % LOG_PERIOD == 0:\n",
    "                print(\n",
    "                    f\"{i}: reward: {reward:.2}, \"\n",
    "                    f\"mean_100: {mean_rewards:.2}, \"\n",
    "                    f\"episodes: {done_episodes}\"\n",
    "                )\n",
    "\n",
    "            writer.add_scalar(\"reward\", reward, i)\n",
    "            writer.add_scalar(\"reward_100\", mean_rewards, i)\n",
    "            writer.add_scalar(\"episodes\", done_episodes, i)\n",
    "\n",
    "            if mean_rewards > REWARD_BOUND:\n",
    "                print(f\"Solved in {i} steps and {done_episodes} episodes!\")\n",
    "                break\n",
    "\n",
    "        # Wait for the batch to fill up\n",
    "        if len(batch_states) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        # Convert batch to tensors\n",
    "        states = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_scale = torch.FloatTensor(batch_scales)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the policy part of the loss function\n",
    "        logits = net(states)\n",
    "        log_action_prob = nn.functional.log_softmax(logits, dim=1)\n",
    "        exp_values = (\n",
    "            batch_scale * log_action_prob[range(BATCH_SIZE), batch_actions_t]\n",
    "        )\n",
    "        policy_loss = -exp_values.mean()\n",
    "\n",
    "        # Compute the entropy bonus to the loss function\n",
    "        action_prob = nn.functional.softmax(logits, dim=1)\n",
    "        entropy = -(action_prob * log_action_prob).sum(dim=1).mean()\n",
    "        entropy_loss = -ENTROPY_BETA * entropy\n",
    "        loss = policy_loss + entropy_loss\n",
    "\n",
    "        # Compute gradient of the loss function and apply one optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute KL divergence: D(previous policy || new policy)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            new_logits = net(states)\n",
    "            new_action_prob = nn.functional.softmax(new_logits, dim=1)\n",
    "\n",
    "            kl_div = (\n",
    "                -(action_prob * (new_action_prob / action_prob).log())\n",
    "                .sum(dim=1)\n",
    "                .mean()\n",
    "            )\n",
    "\n",
    "        # Record KL divergence in TensorBoard\n",
    "        writer.add_scalar(\"kl_div\", kl_div.item(), i)\n",
    "\n",
    "        # Compute additional gradient metrics: max and l2 norms\n",
    "\n",
    "        grad_max = 0.0\n",
    "        grad_means = 0.0\n",
    "        grad_count = 0\n",
    "\n",
    "        for p in net.parameters():\n",
    "            grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "            grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "            grad_count += 1\n",
    "\n",
    "        # Do smooth updates to tracked metrics\n",
    "        #  - Note: We use mixing hyperparameter alpha = 0.95\n",
    "        bs_smoothed = smooth(bs_smoothed, np.mean(batch_scales))\n",
    "        entropy = smooth(entropy, entropy.item())\n",
    "        l_entropy = smooth(l_entropy, entropy_loss.item())\n",
    "        l_policy = smooth(l_policy, policy_loss.item())\n",
    "        l_total = smooth(l_total, loss.item())\n",
    "\n",
    "        # Record metrics for TensorBoard\n",
    "        writer.add_scalar(\"baseline\", baseline, i)\n",
    "        writer.add_scalar(\"entropy\", entropy, i)\n",
    "        writer.add_scalar(\"loss_entropy\", l_entropy, i)\n",
    "        writer.add_scalar(\"loss_policy\", l_policy, i)\n",
    "        writer.add_scalar(\"loss_total\", l_total, i)\n",
    "        writer.add_scalar(\"grad_l2\", grad_means / grad_count, i)\n",
    "        writer.add_scalar(\"grad_max\", grad_max, i)\n",
    "        writer.add_scalar(\"batch_scales\", bs_smoothed, i)\n",
    "\n",
    "        # Batch cleanup\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_scales.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-evaluation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
