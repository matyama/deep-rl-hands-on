{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gothic-allowance",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/matyama/deep-rl-hands-on/blob/main/11_policy_gradients.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "        Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sweet-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "\n",
    "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
    "pip install ptan>=0.7 pytorch-ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-aspect",
   "metadata": {},
   "source": [
    "## Values and Policy\n",
    "Contrary to the value iteration methods (Q-Learning) which try to estimate the state values (state-action values), the *policy gradient* technique focus directly on the policy $\\pi(s)$. \n",
    "\n",
    "Direct policy modeling has several advantages:\n",
    "* From certain point of view, we don't care that much about the expected discounted rewards but rather the decision/action $\\pi(s)$ to take in each state $s$\n",
    "* As we saw earlier with the *Categorical DQN*, learning a distribution helps to better capture the underlying MDP (especially in stochastic environments)\n",
    "* It becomes quite a hard to determine the best action to take when the action space is large or even continuous. The DQN model of $Q(s, a)$ is highly non-linear and the optimization problem $a^* = argmax_a Q(s, a)$ can be hard to solve.\n",
    "\n",
    "In the value iteration case our DQN parametrized the state-action values as $DQN(s) \\to Q_\\mathbf{w}(s, \\cdot)$. Similarly, we will represent the policy as a probability distribution over actions $\\pi_\\mathbf{w}(s)$ parametrized by the NN.\n",
    "\n",
    "*Modelling the output as action (class) probabilities is a typical technique in classification tasks that gives us a smooth representation (intuitively, changing NN weights $\\mathbf{w}$ a bit changes $\\pi$ a bit as well - compared to the case with discrete action labels which would change in steps).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-garbage",
   "metadata": {},
   "source": [
    "## Gradients of the Policy\n",
    "\n",
    "*Policy Gradient* methods are closely related to the *Cross-Entropy Method* introduced earlier. The gradient is a direction in which we want to change NN weights to maximize the accumulated reward and is proportional in scale to the $Q$ state-action value and in the direction to the log of action probabilities:\n",
    "$$\n",
    "\\nabla J \\approx \\mathbb{E}[Q(s, a) \\nabla \\log(\\pi(a | s))]\n",
    "$$\n",
    "where the expectation means that we average the gradient over several steps.\n",
    "\n",
    "Equivalently we can say that we optimize the loss function $\\mathcal{L} = -Q(s, a) \\log(\\pi(a | s))$ (Note: SGD minimizes the loss function but we want to maximize the gradient, therefore the minus sign).\n",
    "\n",
    "Recall that in the *Cross-Entropy Method* we sampled the environment for few episodes and trained only on transitions from the above-average ones. This corresponds to having $Q(s, a) = 1$ for the good transitions and $Q(s, a) = 0$ otherwise. In general, policy gradient methods differ in the way how $Q$ values are treated but in any case we want to use $Q(s, a) \\in [0, 1]$:\n",
    "1. for better separation of episode\n",
    "1. to incorporate the discount factor and thus the uncertainty about future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-village",
   "metadata": {},
   "source": [
    "## The REINFORCE method\n",
    "The outline of the *REINFORCE* methods is the following:\n",
    "1. Initialize NN weights randomly\n",
    "1. Play $N$ full episode and collect experiences $(s, a, r, s')$\n",
    "1. Compute actual $Q$ values for every played episode $k$ and step $t$: $Q_{k, t} = \\sum_{i=0}^t \\gamma^t r_t$\n",
    "1. Compute the loss for all transitions: $\\mathcal{L} = - \\sum_{k, t} Q_{k, t} \\log(\\pi(s_{k, t}, a_{k, t}))$\n",
    "1. Do one SGD step by minimizing the loss and update NN weights\n",
    "1. Repeat from step 2. until convergence\n",
    "\n",
    "Properties of the REINFORCE method:\n",
    "* We **don't need an explicit exploration policy** because we explore automatically using the policy our NN outputs.\n",
    "* **On-policy** method, therefore no ER buffer is needed because we can't train on the data from old policies. On the other hand, value methods typically converge faster (need less interations with the environment).\n",
    "* We train on actual Q values and not estimated ones so we **don't need a target NN** to break experience correlations either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-visiting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
