{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "perceived-physiology",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/matyama/deep-rl-hands-on/blob/main/11_policy_gradients.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "        Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "organizational-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "\n",
    "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
    "pip install ptan>=0.7 pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-crack",
   "metadata": {},
   "source": [
    "## Values and Policy\n",
    "Contrary to the value iteration methods (Q-Learning) which try to estimate the state values (state-action values), the *policy gradient* technique focus directly on the policy $\\pi(s)$. \n",
    "\n",
    "Direct policy modeling has several advantages:\n",
    "* From certain point of view, we don't care that much about the expected discounted rewards but rather the decision/action $\\pi(s)$ to take in each state $s$\n",
    "* As we saw earlier with the *Categorical DQN*, learning a distribution helps to better capture the underlying MDP (especially in stochastic environments)\n",
    "* It becomes quite a hard to determine the best action to take when the action space is large or even continuous. The DQN model of $Q(s, a)$ is highly non-linear and the optimization problem $a^* = argmax_a Q(s, a)$ can be hard to solve.\n",
    "\n",
    "In the value iteration case our DQN parametrized the state-action values as $DQN(s) \\to Q_\\mathbf{w}(s, \\cdot)$. Similarly, we will represent the policy as a probability distribution over actions $\\pi_\\mathbf{w}(s)$ parametrized by the NN.\n",
    "\n",
    "*Modelling the output as action (class) probabilities is a typical technique in classification tasks that gives us a smooth representation (intuitively, changing NN weights $\\mathbf{w}$ a bit changes $\\pi$ a bit as well - compared to the case with discrete action labels which would change in steps).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-liverpool",
   "metadata": {},
   "source": [
    "## Gradients of the Policy\n",
    "\n",
    "*Policy Gradient* methods are closely related to the *Cross-Entropy Method* introduced earlier. The gradient is a direction in which we want to change NN weights to maximize the accumulated reward and is proportional in scale to the $Q$ state-action value and in the direction to the log of action probabilities:\n",
    "$$\n",
    "\\nabla J \\approx \\mathbb{E}[Q(s, a) \\nabla \\log(\\pi(a | s))]\n",
    "$$\n",
    "where the expectation means that we average the gradient over several steps.\n",
    "\n",
    "Equivalently we can say that we optimize the loss function $\\mathcal{L} = -Q(s, a) \\log(\\pi(a | s))$ (Note: SGD minimizes the loss function but we want to maximize the gradient, therefore the minus sign).\n",
    "\n",
    "Recall that in the *Cross-Entropy Method* we sampled the environment for few episodes and trained only on transitions from the above-average ones. This corresponds to having $Q(s, a) = 1$ for the good transitions and $Q(s, a) = 0$ otherwise. In general, policy gradient methods differ in the way how $Q$ values are treated but in any case we want to use $Q(s, a) \\in [0, 1]$:\n",
    "1. for better separation of episode\n",
    "1. to incorporate the discount factor and thus the uncertainty about future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-marriage",
   "metadata": {},
   "source": [
    "## The REINFORCE method\n",
    "The outline of the *REINFORCE* methods is the following:\n",
    "1. Initialize NN weights randomly\n",
    "1. Play $N$ full episode and collect experiences $(s, a, r, s')$\n",
    "1. Compute actual $Q$ values for every played episode $k$ and step $t$: $Q_{k, t} = \\sum_{i=0}^t \\gamma^t r_t$\n",
    "1. Compute the loss for all transitions: $\\mathcal{L} = - \\sum_{k, t} Q_{k, t} \\log(\\pi(s_{k, t}, a_{k, t}))$\n",
    "1. Do one SGD step by minimizing the loss and update NN weights\n",
    "1. Repeat from step 2. until convergence\n",
    "\n",
    "Properties of the REINFORCE method:\n",
    "* We **don't need an explicit exploration policy** because we explore automatically using the policy our NN outputs.\n",
    "* **On-policy** method, therefore no ER buffer is needed because we can't train on the data from old policies. On the other hand, value methods typically converge faster (need less interations with the environment).\n",
    "* We train on actual Q values and not estimated ones so we **don't need a target NN** to break experience correlations either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-trance",
   "metadata": {},
   "source": [
    "### CartPole REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interesting-trauma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "144: reward:  13.00, mean_100:  14.40, episodes: 10\n",
      "319: reward:  17.00, mean_100:  15.95, episodes: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matyama/.cache/pypoetry/virtualenvs/deep-rl-hands-on-A12lkhIw-py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537: reward:  15.00, mean_100:  17.90, episodes: 30\n",
      "887: reward:  55.00, mean_100:  22.18, episodes: 40\n",
      "1349: reward:  51.00, mean_100:  26.98, episodes: 50\n",
      "1815: reward:  41.00, mean_100:  30.25, episodes: 60\n",
      "2509: reward:  97.00, mean_100:  35.84, episodes: 70\n",
      "3107: reward:  38.00, mean_100:  38.84, episodes: 80\n",
      "3545: reward:  38.00, mean_100:  39.39, episodes: 90\n",
      "3918: reward:  26.00, mean_100:  39.18, episodes: 100\n",
      "4184: reward:  37.00, mean_100:  40.40, episodes: 110\n",
      "4483: reward:  28.00, mean_100:  41.64, episodes: 120\n",
      "4776: reward:  24.00, mean_100:  42.39, episodes: 130\n",
      "5088: reward:  46.00, mean_100:  42.01, episodes: 140\n",
      "5535: reward:  53.00, mean_100:  41.86, episodes: 150\n",
      "6082: reward:  64.00, mean_100:  42.67, episodes: 160\n",
      "6763: reward:  52.00, mean_100:  42.54, episodes: 170\n",
      "7421: reward:  56.00, mean_100:  43.14, episodes: 180\n",
      "8046: reward:  34.00, mean_100:  45.01, episodes: 190\n",
      "8739: reward:  88.00, mean_100:  48.21, episodes: 200\n",
      "9418: reward:  30.00, mean_100:  52.34, episodes: 210\n",
      "10014: reward:  99.00, mean_100:  55.31, episodes: 220\n",
      "10793: reward:  56.00, mean_100:  60.17, episodes: 230\n",
      "11477: reward:  50.00, mean_100:  63.89, episodes: 240\n",
      "12398: reward:  81.00, mean_100:  68.63, episodes: 250\n",
      "13310: reward:  93.00, mean_100:  72.28, episodes: 260\n",
      "14320: reward:  63.00, mean_100:  75.57, episodes: 270\n",
      "15211: reward:  82.00, mean_100:  77.90, episodes: 280\n",
      "16529: reward: 159.00, mean_100:  84.83, episodes: 290\n",
      "17835: reward: 200.00, mean_100:  90.96, episodes: 300\n",
      "19273: reward: 200.00, mean_100:  98.55, episodes: 310\n",
      "20952: reward: 200.00, mean_100: 109.38, episodes: 320\n",
      "22412: reward: 200.00, mean_100: 116.19, episodes: 330\n",
      "24167: reward: 200.00, mean_100: 126.90, episodes: 340\n",
      "25932: reward: 200.00, mean_100: 135.34, episodes: 350\n",
      "27109: reward: 112.00, mean_100: 137.99, episodes: 360\n",
      "27856: reward:  75.00, mean_100: 135.36, episodes: 370\n",
      "28499: reward:  57.00, mean_100: 132.88, episodes: 380\n",
      "29266: reward:  48.00, mean_100: 127.37, episodes: 390\n",
      "30145: reward: 130.00, mean_100: 123.10, episodes: 400\n",
      "31468: reward: 156.00, mean_100: 121.95, episodes: 410\n",
      "33331: reward: 200.00, mean_100: 123.79, episodes: 420\n",
      "35331: reward: 200.00, mean_100: 129.19, episodes: 430\n",
      "37331: reward: 200.00, mean_100: 131.64, episodes: 440\n",
      "39331: reward: 200.00, mean_100: 133.99, episodes: 450\n",
      "41331: reward: 200.00, mean_100: 142.22, episodes: 460\n",
      "43331: reward: 200.00, mean_100: 154.75, episodes: 470\n",
      "45331: reward: 200.00, mean_100: 168.32, episodes: 480\n",
      "47327: reward: 196.00, mean_100: 180.61, episodes: 490\n",
      "49327: reward: 200.00, mean_100: 191.82, episodes: 500\n",
      "Solved in 50127 steps and 504 episodes!\n"
     ]
    }
   ],
   "source": [
    "# flake8: noqa: E402,I001\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ptan\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ptan.experience import ExperienceFirstLast\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "class PGN(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Gradient Network that consumes states (observations)\n",
    "    and outputs action logits (scores).\n",
    "      - Note: Logits should be manually converted to probabilities\n",
    "        with `log_softmax` for better numerical stability and optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: Tuple[int, ...], n_actions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Simple, not really deep, forward network that outputs action logits\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def compute_q_values(rewards: List[float], gamma: float) -> Iterable[float]:\n",
    "    qs = []\n",
    "    sum_r = 0.0\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= gamma\n",
    "        sum_r += r\n",
    "        qs.append(sum_r)\n",
    "\n",
    "    return reversed(qs)\n",
    "\n",
    "\n",
    "def train_reinforce(\n",
    "    env_name: str,\n",
    "    gamma: float = 0.99,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_played_episodes: int = 4,\n",
    "    reward_bound: int = 195,\n",
    "    log_period: int = 10,\n",
    ") -> None:\n",
    "\n",
    "    # Crate the environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Create PG network\n",
    "    net = PGN(\n",
    "        input_shape=env.observation_space.shape[0],\n",
    "        n_actions=env.action_space.n,\n",
    "    )\n",
    "    print(net)\n",
    "\n",
    "    # Initialize an agent\n",
    "    #  - Notice: We instruct it to apply softmax to the PGN output\n",
    "    agent = ptan.agent.PolicyAgent(\n",
    "        net,\n",
    "        preprocessor=ptan.agent.float32_preprocessor,\n",
    "        apply_softmax=True,\n",
    "    )\n",
    "\n",
    "    # Create experience source and optimizer\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    with SummaryWriter(comment=f\"-{env_name}-reinforce\") as writer:\n",
    "\n",
    "        done_episodes = 0\n",
    "        batch_episodes = 0\n",
    "\n",
    "        batch_states, batch_actions, batch_q_values = [], [], []\n",
    "\n",
    "        episode_rewards = []\n",
    "        total_rewards = []\n",
    "\n",
    "        # Interact with the environment and consume experiences\n",
    "        for i, exp in enumerate(exp_source):\n",
    "\n",
    "            # Add the new experience to current batch\n",
    "            batch_states.append(exp.state)\n",
    "            batch_actions.append(int(exp.action))\n",
    "\n",
    "            # Buffer immedieate rewards during each episode\n",
    "            episode_rewards.append(exp.reward)\n",
    "\n",
    "            # Compute Q values from immediate rewards when episode ends\n",
    "            if exp.last_state is None:\n",
    "                batch_q_values += compute_q_values(episode_rewards, gamma)\n",
    "                episode_rewards.clear()\n",
    "                batch_episodes += 1\n",
    "\n",
    "            # Handle new rewards\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "\n",
    "                done_episodes += 1\n",
    "\n",
    "                # Collect total rewards\n",
    "                reward = new_rewards[0]\n",
    "                total_rewards.append(reward)\n",
    "\n",
    "                # Compute the mean reward over last 100 episodes\n",
    "                mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "\n",
    "                # Log training progress\n",
    "                if done_episodes % log_period == 0:\n",
    "                    print(\n",
    "                        f\"{i}: reward: {reward:.2}, \"\n",
    "                        f\"mean_100: {mean_rewards:.2}, \"\n",
    "                        f\"episodes: {done_episodes}\"\n",
    "                    )\n",
    "\n",
    "                # Record metrics for TensorBoard\n",
    "                writer.add_scalar(\"reward\", reward, i)\n",
    "                writer.add_scalar(\"reward_100\", mean_rewards, i)\n",
    "                writer.add_scalar(\"episodes\", done_episodes, i)\n",
    "\n",
    "                # Check if the learned policy is good enough\n",
    "                if mean_rewards > reward_bound:\n",
    "                    print(f\"Solved in {i} steps and {done_episodes} episodes!\")\n",
    "                    break\n",
    "\n",
    "            # Play N episodes to accumulate Q values before training step\n",
    "            if batch_episodes < n_played_episodes:\n",
    "                continue\n",
    "\n",
    "            n_states = len(batch_states)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Convert batch parts to tensors\n",
    "            states = torch.FloatTensor(batch_states)\n",
    "            actions = torch.LongTensor(batch_actions)\n",
    "            q_values = torch.FloatTensor(batch_q_values)\n",
    "\n",
    "            # Compute action scores (logits)\n",
    "            #  - Note: There's just single pass through the PGN (DQN has 2)\n",
    "            logits = net(states)\n",
    "\n",
    "            # Compute the loss funciton defiend in previous section\n",
    "            log_action_prob = nn.functional.log_softmax(logits, dim=1)\n",
    "            exp_values = q_values * log_action_prob[range(n_states), actions]\n",
    "            loss = -exp_values.mean()\n",
    "\n",
    "            # Compute gradient of the loss function and make one SGD step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reset current batch\n",
    "            batch_episodes = 0\n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_q_values.clear()\n",
    "\n",
    "\n",
    "# Run REINFORCE to solve the CartPole environment\n",
    "train_reinforce(env_name=\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-separation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
