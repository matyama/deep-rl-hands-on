{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "muslim-manchester",
   "metadata": {},
   "source": [
    "# Tabular Learning and the Bellman Equation\n",
    "Given a MDP $(S, A, T, R, \\gamma)$, the optimal state values $V(s)$ and state action values $Q(s, a)$ are described by the *Bellman Equation*:\n",
    "$$\n",
    "V(s) = \\max_{a \\in A} Q(s, a) = \\max_{a \\in A} \\sum_{s' \\in S} T(s, a, s') (R(s, a, s') + \\gamma V(s'))\n",
    "$$\n",
    "where\n",
    "* $T(s, a, s') \\in [0, 1]$ is a stochastic transition model, i.e. the probability $P(s' | s, a)$ of reaching state $s' \\in S$ from $s \\in S$ with action $a \\in A$\n",
    "* $R(s, a, s')$ is the immediate reward of transitioning from $s$ to $s'$ using action $a$. Note that this may collapse, e.g. to $R(s')$.\n",
    "* $\\gamma \\in (0, 1]$ is a discount factor for future rewards\n",
    "\n",
    "Deterministic optimal policy then plays action $\\pi(s) = argmax_{a \\in A} Q(s, a)$. A deterministic policy can be found given complete knowledge of the environment, that is the transition model $T$ and rewards $R$. Otherwise one has to either build up a model of these or use other learning techiques.\n",
    "\n",
    "*Value iteration* method then simply starts with $V_0(s) \\gets 0$ and uses the Bellman update to compute $V_{k + 1}(s)$ using $V_k(s)$. *Q-value iteration* then simply adopts this approach for Q values (note that $V(s') = \\max_{a'} Q(s', a')$ in the Bellman update)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-modern",
   "metadata": {},
   "source": [
    "## FrozenLake: Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bronze-latvia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 45 iterations with best reward: 0.950\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Type\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnvModel:\n",
    "    transits: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def new(cls, n_states: int, n_actions: int) -> \"EnvModel\":\n",
    "        shape = (n_states, n_actions, n_states)\n",
    "        return cls(\n",
    "            transits=np.zeros(shape=shape, dtype=np.uint32),\n",
    "            rewards=np.zeros(shape=shape, dtype=np.float64),\n",
    "        )\n",
    "\n",
    "    def add_experience(self, s: int, a: int, r: float, sp: int) -> None:\n",
    "        self.transits[s, a, sp] += 1\n",
    "        self.rewards[s, a, sp] = r\n",
    "\n",
    "    @property\n",
    "    def tranistion_probas(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes empirical distribution over next states for each (s, a).\n",
    "\n",
    "        Note: Transitions (s, a) that have not yet been played are set to 0.\n",
    "        \"\"\"\n",
    "        totals = np.sum(self.transits, axis=2)[..., np.newaxis]\n",
    "        probas = np.zeros_like(self.transits, dtype=np.float64)\n",
    "        return np.divide(self.transits, totals, out=probas, where=totals > 0)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: int,\n",
    "        n_states: int,\n",
    "        n_actions: int,\n",
    "        gamma: float,\n",
    "    ) -> None:\n",
    "        self.state = state\n",
    "        self.model = EnvModel.new(n_states, n_actions)\n",
    "        self.gamma = gamma\n",
    "        self.values = np.zeros(n_states, dtype=np.float64)\n",
    "\n",
    "    @classmethod\n",
    "    def init(cls, env: gym.Env, gamma: float) -> \"Agent\":\n",
    "        return cls(\n",
    "            state=env.reset(),\n",
    "            n_states=env.observation_space.n,\n",
    "            n_actions=env.action_space.n,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def q_values(self) -> np.ndarray:\n",
    "        targets = self.model.rewards + self.gamma * self.values\n",
    "        return np.sum(self.model.tranistion_probas * targets, axis=2)\n",
    "\n",
    "    def policy(self, state: int) -> int:\n",
    "        return np.argmax(self.q_values[state])\n",
    "\n",
    "    def _update_values(self) -> None:\n",
    "        self.values = np.max(self.q_values, axis=1)\n",
    "\n",
    "    def explore(self, env: gym.Env, n_steps: int) -> None:\n",
    "        \"\"\"\n",
    "        Explore given environment for a bit and update policy\n",
    "        based on collected experience.\n",
    "        \"\"\"\n",
    "\n",
    "        # Improve the environment model based on experiences\n",
    "        for _ in range(n_steps):\n",
    "\n",
    "            # Use random policy for exploration\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # Gain new experience by interacting with the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            self.model.add_experience(self.state, action, reward, next_state)\n",
    "\n",
    "            # Update internal state\n",
    "            self.state = env.reset() if done else next_state\n",
    "\n",
    "        # Adjust values using the Bellman update\n",
    "        self._update_values()\n",
    "\n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Use learned policy to select best action in current state\n",
    "            action = self.policy(state)\n",
    "\n",
    "            # Apply the action and collect new experience\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            self.model.add_experience(state, action, reward, next_state)\n",
    "\n",
    "            # Accumulate non-discounted reward for the episode\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def exploit(self, env: gym.Env, n_episodes: int) -> float:\n",
    "        \"\"\"\n",
    "        Play several episodes in given environment and return\n",
    "        average non-discounted reward.\n",
    "\n",
    "        Note: This does not update agent's state, except the environment model\n",
    "        \"\"\"\n",
    "        total_reward = sum(self.play_episode(env) for _ in range(n_episodes))\n",
    "        return total_reward / n_episodes\n",
    "\n",
    "\n",
    "def train(\n",
    "    agent_cls: Type[Agent],\n",
    "    train_env: gym.Env,\n",
    "    test_episodes: int = 20,\n",
    "    explore_steps: int = 100,\n",
    "    solution_bound: float = 0.8,\n",
    "    max_iters: int = 1000,\n",
    "    gamma: float = 0.9,\n",
    ") -> None:\n",
    "    with SummaryWriter(comment=\"-v-iteration\") as writer:\n",
    "\n",
    "        # Create a sandboxed copy of the environment for evaluation\n",
    "        test_env = gym.make(train_env.spec.id)\n",
    "\n",
    "        # Initialize an agent\n",
    "        agent = agent_cls.init(train_env, gamma)\n",
    "\n",
    "        i = 0\n",
    "        reward = 0.0\n",
    "        best_reward = 0.0\n",
    "\n",
    "        while reward < solution_bound and i < max_iters:\n",
    "            i += 1\n",
    "\n",
    "            # Improve the policy by interacting with the environment\n",
    "            agent.explore(env=train_env, n_steps=explore_steps)\n",
    "\n",
    "            # Run some trials in sandboxed environment\n",
    "            #  - Note: We collect new experience here too.\n",
    "            reward = agent.exploit(env=test_env, n_episodes=test_episodes)\n",
    "\n",
    "            # Update best overall reward\n",
    "            best_reward = max(best_reward, reward)\n",
    "\n",
    "            # Record rewards for monitoring\n",
    "            writer.add_scalar(\"reward\", reward, i)\n",
    "            writer.add_scalar(\"best_reward\", best_reward, i)\n",
    "\n",
    "    # Show final result\n",
    "    print(f\"Solved in {i} iterations with best reward: {best_reward:.3f}\")\n",
    "\n",
    "\n",
    "# Train an agent using Value Iteration\n",
    "train(Agent, train_env=gym.make(\"FrozenLake8x8-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-burden",
   "metadata": {},
   "source": [
    "## FrozenLake: Q-Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expensive-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 326 iterations with best reward: 0.800\n"
     ]
    }
   ],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: int,\n",
    "        n_states: int,\n",
    "        n_actions: int,\n",
    "        gamma: float,\n",
    "    ) -> None:\n",
    "        super().__init__(state, n_states, n_actions, gamma)\n",
    "        self.values = np.zeros(shape=(n_states, n_actions), dtype=np.float64)\n",
    "\n",
    "    @property\n",
    "    def q_values(self) -> np.ndarray:\n",
    "        next_state_values = np.max(self.values, axis=1)\n",
    "        targets = self.model.rewards + self.gamma * next_state_values\n",
    "        return np.sum(self.model.tranistion_probas * targets, axis=2)\n",
    "\n",
    "    def policy(self, state: int) -> int:\n",
    "        return np.argmax(self.values[state])\n",
    "\n",
    "    def _update_values(self) -> None:\n",
    "        self.values = self.q_values\n",
    "\n",
    "\n",
    "# Train an agent using Q-Value Iteration\n",
    "train(QAgent, train_env=gym.make(\"FrozenLake8x8-v0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-receptor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
