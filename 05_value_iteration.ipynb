{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "honey-apparatus",
   "metadata": {},
   "source": [
    "# Tabular Learning and the Bellman Equation\n",
    "Given a MDP $(S, A, T, R, \\gamma)$, the optimal state values $V(s)$ and state action values $Q(s, a)$ are described by the *Bellman Equation*:\n",
    "$$\n",
    "V(s) = \\max_{a \\in A} Q(s, a) = \\max_{a \\in A} \\sum_{s' \\in S} T(s, a, s') (R(s, a, s') + \\gamma V(s'))\n",
    "$$\n",
    "where\n",
    "* $T(s, a, s') \\in [0, 1]$ is a stochastic transition model, i.e. the probability $P(s' | s, a)$ of reaching state $s' \\in S$ from $s \\in S$ with action $a \\in A$\n",
    "* $R(s, a, s')$ is the immediate reward of transitioning from $s$ to $s'$ using action $a$. Note that this may collapse, e.g. to $R(s')$.\n",
    "* $\\gamma \\in (0, 1]$ is a discount factor for future rewards\n",
    "\n",
    "Deterministic optimal policy then plays action $\\pi(s) = argmax_{a \\in A} Q(s, a)$. A deterministic policy can be found given complete knowledge of the environment, that is the transition model $T$ and rewards $R$. Otherwise one has to either build up a model of these or use other learning techiques.\n",
    "\n",
    "*Value iteration* method then simply starts with $V_0(s) \\gets 0$ and uses the Bellman update to compute $V_{k + 1}(s)$ using $V_k(s)$. *Q-value iteration* then simply adopts this approach for Q values (note that $V(s') = \\max_{a'} Q(s', a')$ in the Bellman update)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-spelling",
   "metadata": {},
   "source": [
    "## FrozenLake: Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "novel-auditor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 131 iterations with best reward: 0.800\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnvModel:\n",
    "    transits: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def new(cls, n_states: int, n_actions: int) -> \"EnvModel\":\n",
    "        shape = (n_states, n_actions, n_states)\n",
    "        return cls(\n",
    "            transits=np.zeros(shape=shape, dtype=np.uint32),\n",
    "            rewards=np.zeros(shape=shape, dtype=np.float64),\n",
    "        )\n",
    "\n",
    "    def add_experience(self, s: int, a: int, r: float, sp: int) -> None:\n",
    "        self.transits[s, a, sp] += 1\n",
    "        self.rewards[s, a, sp] = r\n",
    "\n",
    "    @property\n",
    "    def tranistion_probas(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes empirical distribution over next states for each (s, a).\n",
    "\n",
    "        Note: Transitions (s, a) that have not yet been played are set to 0.\n",
    "        \"\"\"\n",
    "        totals = np.sum(self.transits, axis=2)[..., np.newaxis]\n",
    "        probas = np.zeros_like(self.transits, dtype=np.float64)\n",
    "        return np.divide(self.transits, totals, out=probas, where=totals > 0)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: int,\n",
    "        n_states: int,\n",
    "        n_actions: int,\n",
    "        gamma: float,\n",
    "    ) -> None:\n",
    "        self.state = state\n",
    "        self.model = EnvModel.new(n_states, n_actions)\n",
    "        self.gamma = gamma\n",
    "        self.values = np.zeros(n_states, dtype=np.float64)\n",
    "\n",
    "    @classmethod\n",
    "    def init(cls, env: gym.Env, gamma: float) -> \"Agent\":\n",
    "        return cls(\n",
    "            state=env.reset(),\n",
    "            n_states=env.observation_space.n,\n",
    "            n_actions=env.action_space.n,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def q_values(self) -> np.ndarray:\n",
    "        targets = self.model.rewards + self.gamma * self.values\n",
    "        return np.sum(self.model.tranistion_probas * targets, axis=2)\n",
    "\n",
    "    def policy(self, state: int) -> int:\n",
    "        return np.argmax(self.q_values[state])\n",
    "\n",
    "    def explore(self, env: gym.Env, n_steps: int) -> None:\n",
    "        \"\"\"\n",
    "        Explore given environment for a bit and update policy\n",
    "        based on collected experience.\n",
    "        \"\"\"\n",
    "\n",
    "        # Improve the environment model based on experiences\n",
    "        for _ in range(n_steps):\n",
    "\n",
    "            # Use random policy for exploration\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # Gain new experience by interacting with the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            self.model.add_experience(self.state, action, reward, next_state)\n",
    "\n",
    "            # Update internal state\n",
    "            self.state = env.reset() if done else next_state\n",
    "\n",
    "        # Adjust values using the Bellman update\n",
    "        self.values = np.max(self.q_values, axis=1)\n",
    "\n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Use learned policy to select best action in current state\n",
    "            action = self.policy(state)\n",
    "\n",
    "            # Apply the action and collect new experience\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            self.model.add_experience(state, action, reward, next_state)\n",
    "\n",
    "            # Accumulate non-discounted reward for the episode\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def exploit(self, env: gym.Env, n_episodes: int) -> float:\n",
    "        \"\"\"\n",
    "        Play several episodes in given environment and return\n",
    "        average non-discounted reward.\n",
    "\n",
    "        Note: This does not update agent's state, except the environment model\n",
    "        \"\"\"\n",
    "        total_reward = sum(self.play_episode(env) for _ in range(n_episodes))\n",
    "        return total_reward / n_episodes\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "TEST_EPISODES = 20\n",
    "EXPLORE_STEPS = 100\n",
    "SOLUTION_BOUND = 0.8\n",
    "MAX_ITERS = 1_000\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Crate two environments\n",
    "#  - First one is for exploration and training\n",
    "#  - The other is a sandpox for evaluation\n",
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "test_env = gym.make(\"FrozenLake8x8-v0\")\n",
    "\n",
    "# Initialize an agent\n",
    "agent = Agent.init(env, gamma=GAMMA)\n",
    "\n",
    "with SummaryWriter(comment=\"-v-iteration\") as writer:\n",
    "\n",
    "    i = 0\n",
    "    reward = 0.0\n",
    "    best_reward = 0.0\n",
    "\n",
    "    while reward < SOLUTION_BOUND and i < MAX_ITERS:\n",
    "        i += 1\n",
    "\n",
    "        # Improve the policy by interacting with the environment\n",
    "        agent.explore(env=env, n_steps=EXPLORE_STEPS)\n",
    "\n",
    "        # Run some trials in sandboxed environment\n",
    "        #  - Note: We collect new experience here too.\n",
    "        reward = agent.exploit(env=test_env, n_episodes=TEST_EPISODES)\n",
    "\n",
    "        # Update best overall reward\n",
    "        best_reward = max(best_reward, reward)\n",
    "\n",
    "        # Record rewards for monitoring\n",
    "        writer.add_scalar(\"reward\", reward, i)\n",
    "        writer.add_scalar(\"best_reward\", best_reward, i)\n",
    "\n",
    "# Show final result\n",
    "print(f\"Solved in {i} iterations with best reward: {best_reward:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-handling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
