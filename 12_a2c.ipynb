{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fantastic-department",
   "metadata": {},
   "source": [
    "# The Actor-Critic Method\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/matyama/deep-rl-hands-on/blob/main/12_a2c.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "        Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "buried-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "\n",
    "echo \"Running on Google Colab, therefore installing dependencies...\"\n",
    "pip install ptan>=0.7 tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-papua",
   "metadata": {},
   "source": [
    "## Variance Reduction\n",
    "Let's start by recalling the policy gradient defined by the *Policy Gradients (PG)* method:\n",
    "$$\n",
    "\\nabla J \\approx \\mathbb{E}[Q(s, a) \\nabla \\log(\\pi(a|s))]\n",
    "$$\n",
    "\n",
    "One of the weak points of the PG method is that the gradient scales $Q(s, a)$ may experience quite significant variance* which does not help the training at all. We fixed this issue by introducing a fixed *baseline* value (e.g. mean reward) that was subtracted from the gradient scales Q.\n",
    "\n",
    "\\* Recall formal defintion: $\\mathbb{V}[X] = \\mathbb{E}[(X - \\mathbb{E}[X])^2]$\n",
    "\n",
    "Let's illustrate this problem and solution on simple example:\n",
    "* Assume there are three actions with $Q_1$, $Q_2$ some small positive values and $Q_3$ being large negative\n",
    "* In this case there will be small positive gradient towards fist two actions and large negative one repelling the policy from the third one\n",
    "* Now imagine $Q_1$ and $Q_2$ were large positive values instead. Then $Q_3$ would become small but positive value. The gradient would still push the policy towards fist two actions but it would direct the gradient towards the trird one a bit as well (instead of pushing it away from it)!\n",
    "\n",
    "Now it's a bit more clear why subtracting a constant value that we called the *baseline* helps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-leone",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic (A2C)\n",
    "*Advantage Actor-Critic (A2C)* method can be viewed as a combination of PG and DQN with a simple idea extending the variance reduction theme we discussed above. Until now we treated the *baseline* value as single constant that we subtracted from all $Q(s, a)$ values. A2C pushes this further and uses different baselines for each state $s$.\n",
    "\n",
    "If one recalls the *Duelling DQN* which exploited the fact that $Q(s, a) = V(s) + A(s, a)$ - i.e. state-action values are composed of a *baseline* state values $V(s)$ and action advantages $A(s, a)$ in these states, it is quite straightforward to figure out which values A2C uses as state baselines - the state values $V(s)$!\n",
    "\n",
    "The *Advantage* Actor-Critic name then comes from the fact that our gradient scales turn to action advantages after subtracting state values:\n",
    "$$\n",
    "\\nabla J \\approx \\mathbb{E}[Q(s, a) \\nabla \\log(\\pi(a|s))] \\to \\mathbb{E}[A(s, a) \\nabla \\log(\\pi(a|s))]\n",
    "$$\n",
    "\n",
    "Finally, the question is how do we obtain $V(s)$? Here comes the second part which is the combination with the DQN approach - we simply train a DQN alongside our PGN.\n",
    "\n",
    "*Notes*:\n",
    "* *There'll actually be just single NN that will learn both the policy and state values (discussed below)*\n",
    "* *Improvements from both methods are still applicable (also metioned and shown in following sections)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-cisco",
   "metadata": {},
   "source": [
    "### Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mechanical-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa: E402,I001\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Iterable, List, Optional, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ptan\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ptan.experience import ExperienceFirstLast\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-tomorrow",
   "metadata": {},
   "source": [
    "### Reward Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "checked-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTracker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        writer: SummaryWriter,\n",
    "        stop_reward: float,\n",
    "        window_size: int = 100,\n",
    "    ) -> None:\n",
    "        self.writer = writer\n",
    "        self.stop_reward = stop_reward\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __enter__(self) -> \"RewardTracker\":\n",
    "        self.ts = time.time()\n",
    "        self.ts_frame = 0\n",
    "        self.total_rewards = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args: Any) -> None:\n",
    "        self.writer.close()\n",
    "\n",
    "    def add_reward(self, reward: float, frame: int) -> bool:\n",
    "        \"\"\"\n",
    "        Returns an indication of whether a termination contition was reached.\n",
    "        \"\"\"\n",
    "\n",
    "        self.total_rewards.append(reward)\n",
    "\n",
    "        fps = (frame - self.ts_frame) / (time.time() - self.ts)\n",
    "\n",
    "        self.ts_frame = frame\n",
    "        self.ts = time.time()\n",
    "\n",
    "        mean_reward = np.mean(self.total_rewards[-self.window_size :])\n",
    "\n",
    "        if frame % self.window_size == 0:\n",
    "            print(\n",
    "                f\"{frame}: done {len(self.total_rewards)} games, \"\n",
    "                f\"mean reward {mean_reward:.3}, speed {fps:.2} fps\"\n",
    "            )\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        self.writer.add_scalar(\"fps\", fps, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "        self.writer.add_scalar(\"reward\", reward, frame)\n",
    "\n",
    "        return mean_reward > self.stop_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-brake",
   "metadata": {},
   "source": [
    "### Atari A2C PG Network\n",
    "This NN is quite similar to the *Dueling DQN* architecture but with an important difference. In the Dueling DQN we have also two parts\n",
    "1. Part for the state values $V(s)$\n",
    "1. Part for the action advantages $A(s, a)$\n",
    "\n",
    "But as with any other DQN we did still output $Q(s, a) = V(s) + A(s, a)$. Here we have two separate outputs with common base network:\n",
    "1. Policy network that outputs action logits - basically policy $\\pi(a|s)$ when one converts them to probabilities using softmax\n",
    "1. Value network which computes $V(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "killing-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariA2C(nn.Module):\n",
    "    \"\"\"\n",
    "    A2C network with 2D convolutional base for Atari envs. and two heads:\n",
    "    1. Policy - dense network that outputs action logits\n",
    "    2. Value - dence network that models state values `V(s)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: Tuple[int, ...], n_actions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # 2D conv. base network common to both heads\n",
    "        #  - This way both nets share commonly learned basic features\n",
    "        #  - Also helps with convergence (compared to having two separate NNs)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        # Policy NN - outputs action logits\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "        # Value NN - outputs state value\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape: Tuple[int, ...]) -> int:\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        For an input batch of states, returns pair of tensors\n",
    "          1. Policy logits for all actions in these states\n",
    "          2. Values of these states\n",
    "        \"\"\"\n",
    "        inputs = x.float() / 256\n",
    "        conv_out = self.conv(inputs).view(inputs.size()[0], -1)\n",
    "        return self.policy(conv_out), self.value(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-posting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
