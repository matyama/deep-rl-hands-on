{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fuzzy-court",
   "metadata": {},
   "source": [
    "# Deep Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-steering",
   "metadata": {},
   "source": [
    "## Tabular Q-Learning\n",
    "By *Tabular Q-Learning* we mean a *model-free* set of techniques which work in discrete (or discretized) environments with small amount of states and actions and keep around a table of Q values. Contrary to the Q-Value Learning from the last chapter, here we don't explicitly model the environment.\n",
    "\n",
    "After getting an experience $(s, a, r, s')$ we perform a blending Bellman approximation update:\n",
    "$$\n",
    "Q(s, a) \\gets (1 - \\alpha) Q(s, a) + \\alpha (r + \\max_{a'} Q(s', a'))\n",
    "$$\n",
    "which can be reformulated in terms of *Temporal Difference learning (TD learning)* as\n",
    "$$\n",
    "Q(s, a) \\gets Q(s, a) + \\alpha (r + \\max_{a'} Q(s', a') - Q(s, a)) = Q(s, a) + \\alpha \\delta(s, a, r, s')\n",
    "$$\n",
    "where\n",
    "* $\\delta(s, a, r, s') = r + \\max_{a'} Q(s', a') - Q(s, a)$ is called *TD error* and\n",
    "* $r + \\max_{a'} Q(s', a')$ is the *TD target*\n",
    "\n",
    "Finally, for efficiency reasons we don't actually have to construct the full Q table as we don't really care about states that we've never experience. So we'll estimate values only for those states that we've seen and iterate over a smaller set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elementary-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 5000 iterations with best reward: 0.750\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from functools import partial\n",
    "from typing import Callable, NamedTuple, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    state: int\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: int\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Q-Learning agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "    ) -> None:\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def policy(self, s: int) -> int:\n",
    "        \"\"\"Returns the best known action\"\"\"\n",
    "        return np.argmax([self.values[s, a] for a in range(self.n_actions)])\n",
    "\n",
    "    def __iadd__(self, e: Experience) -> \"Agent\":\n",
    "        Q_max = self.values[e.next_state, self.policy(e.next_state)]\n",
    "        td_target = e.reward + self.gamma * Q_max\n",
    "        td_error = td_target - self.values[e.state, e.action]\n",
    "        self.values[e.state, e.action] += self.alpha * td_error\n",
    "        return self\n",
    "\n",
    "\n",
    "def explore(env: gym.Env, state: int) -> Tuple[Experience, int]:\n",
    "    \"\"\"\n",
    "    Samples and applies a random action in given environment from given state.\n",
    "    Returns experience (s, a, r, s') and new state (resets env if necessary).\n",
    "    \"\"\"\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    experience = Experience(state, action, reward, next_state)\n",
    "    state = env.reset() if done else next_state\n",
    "    return experience, state\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    env: gym.Env,\n",
    "    n_episodes: int,\n",
    "    policy: Callable[[int], int],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Runs n episodes in given environment using given policy and\n",
    "    computes the mean non-disounted reward.\n",
    "    \"\"\"\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        episode_done = False\n",
    "\n",
    "        while not episode_done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, episode_done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "    return total_reward / n_episodes\n",
    "\n",
    "\n",
    "def train(\n",
    "    env: gym.Env,\n",
    "    eval_episodes: int = 20,\n",
    "    alpha: float = 0.2,\n",
    "    gamma: float = 0.9,\n",
    "    solution_bound: float = 0.8,\n",
    "    max_iters: int = 5_000,\n",
    ") -> None:\n",
    "    with SummaryWriter(comment=\"-q-learning\") as writer:\n",
    "\n",
    "        # Bind experience sampling and policy evaluation to environments\n",
    "        #  - Note: We use a copy of the env. for testing.\n",
    "        explore_env = partial(explore, env)\n",
    "        eval_policy = partial(evaluate, gym.make(env.spec.id), eval_episodes)\n",
    "\n",
    "        # Initialize the Q-Learning agent\n",
    "        agent = Agent(env.action_space.n, alpha, gamma)\n",
    "\n",
    "        i = 0\n",
    "        reward = 0.0\n",
    "        best_reward = 0.0\n",
    "\n",
    "        # Initialize the environment\n",
    "        state = env.reset()\n",
    "\n",
    "        while reward < solution_bound and i < max_iters:\n",
    "            i += 1\n",
    "\n",
    "            # Sample new experience from the environment\n",
    "            # and pass it to the agent to learn from it.\n",
    "            experience, state = explore_env(state)\n",
    "            agent += experience\n",
    "\n",
    "            # Evaluate current policy\n",
    "            mean_reward = eval_policy(agent.policy)\n",
    "            best_reward = max(mean_reward, best_reward)\n",
    "\n",
    "            # Record metrics\n",
    "            writer.add_scalar(\"reward\", mean_reward, i)\n",
    "            writer.add_scalar(\"best_reward\", best_reward, i)\n",
    "\n",
    "    print(f\"Solved in {i} iterations with best reward: {best_reward:.3f}\")\n",
    "\n",
    "\n",
    "# Run Q-Learning in FrozenLake\n",
    "train(env=gym.make(\"FrozenLake-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-somalia",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "Even Q-Learning does not iterate over all possible states, the size of the Q table might become intractable. The idea of *Deep Q-Learning* is to use a deep NN to represent $Q(s, a) \\approx Q(s, a; \\mathbf{w})$ ($\\mathbf{w}$ are the NN parameters). We treat the problem as a supervised regression task and train the network with a SGD on a dataset of collected experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-canvas",
   "metadata": {},
   "source": [
    "### Problems in Deep Q-Learning\n",
    "Although a NN model is a compact representation of the Q table, the learning dynamics we described above has some issues:\n",
    "\n",
    "#### Exploration-Exploitation Dilema\n",
    "In all the previous examples we've used random actions to sample experiences from the environment (exploration). However, if we're confident enough in the policy we have, it'd be much better to exploit it and use $a = argmax_a Q(s, a)$ in a state $s$.\n",
    "\n",
    "A straightforward trick that is used to balance exploration and exploitation a bit is called $\\epsilon$*-greedy* policy which randomly switches between these two extremes with $\\epsilon$ probability. We typically define a schedule and decrease $\\epsilon$ as the training proceeds.\n",
    "\n",
    "#### SGD Optimization\n",
    "SGD is heavily based on the assumption of the training instances being *i.i.d* which is definitely not the case in Q-Learning dynamics.\n",
    "\n",
    "First, independence is broken due to the fact further states in an episode do depend on previous actions and states.\n",
    "\n",
    "Furthermore, the training data (experiences) are not identically distributed. Regardless of which policy we use to sample the experiences (during training) it won't produce the same distibution of experiences as if we had used the optimal policy. In short, our targets do not follow the same distribution as what we're trying to learn.\n",
    "\n",
    "To mitigate the i.i.d problem we introduce a *replay buffer* - a large collection of experinces from which we sample training mini-batches.\n",
    "\n",
    "#### Experience Correlations\n",
    "Next issue is that the future Q values $Q(s', a')$ are highly correlated with current $Q(s, a)$ that we try to update. The model starts by modifying $Q(s', a')$ which might cause $Q(s, a)$ to get worse but then by trying fixing $Q(s, a)$ we break $Q(s', a')$. Basically, the training can be quite unstable.\n",
    "\n",
    "A technique that is frequently used to address this is described as *fixing (freezing) the targets*. We introduce second NN $\\hat{Q}$ which is a copy of our $Q$ NN and use DT targets $r + \\gamma \\max_{a'} \\hat{Q}(s', a')$. NN parameters are then regularly synchronized with quite a large period (1k or even 10k iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-asbestos",
   "metadata": {},
   "source": [
    "### DQN Training\n",
    "Putting all the pieces together, here is an overview of the training algorithm for a *Deep Q-Network (DQN)*.\n",
    "1. Initialize parameters of $Q$ and $\\hat{Q}$, start with $\\epsilon \\gets 1$\n",
    "1. Select a random action $a$ with $\\epsilon$ probability, otherwise pick $a = argmax_a Q(s, a)$\n",
    "1. Run $a$ in the environment and gain experience $e = (s, a, r, s')$\n",
    "1. Store $e$ in the replay buffer\n",
    "1. Sample random mini-batch $B$ from the replay buffer\n",
    "1. For each $(s, a, r, s') \\in B$ alculate targets $y$: $y = r$ if episode ends, otherwise $y = r + \\gamma \\max_{a'} \\hat{Q}(s', a')$\n",
    "1. Compute the loss $\\mathcal{L} = (Q(s, a) - y)^2$\n",
    "1. Using $\\mathcal{L}$ run a SGD step to update $Q(s, a)$\n",
    "1. Every $N$ steps copy $\\mathbf{w}_Q \\to \\mathbf{w}_{\\hat{Q}}$\n",
    "1. Repeat from 2. until convergence or other termination condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
