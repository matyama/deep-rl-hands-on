{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rocky-circulation",
   "metadata": {},
   "source": [
    "# Deep Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-lawsuit",
   "metadata": {},
   "source": [
    "## Tabular Q-Learning\n",
    "By *Tabular Q-Learning* we mean a *model-free* set of techniques which work in discrete (or discretized) environments with small amount of states and actions and keep around a table of Q values. Contrary to the Q-Value Learning from the last chapter, here we don't explicitly model the environment.\n",
    "\n",
    "After getting an experience $(s, a, r, s')$ we perform a blending Bellman approximation update:\n",
    "$$\n",
    "Q(s, a) \\gets (1 - \\alpha) Q(s, a) + \\alpha (r + \\max_{a'} Q(s', a'))\n",
    "$$\n",
    "which can be reformulated in terms of *Temporal Difference learning (TD learning)* as\n",
    "$$\n",
    "Q(s, a) \\gets Q(s, a) + \\alpha (r + \\max_{a'} Q(s', a') - Q(s, a)) = Q(s, a) + \\alpha \\delta(s, a, r, s')\n",
    "$$\n",
    "where\n",
    "* $\\delta(s, a, r, s') = r + \\max_{a'} Q(s', a') - Q(s, a)$ is called *TD error* and\n",
    "* $r + \\max_{a'} Q(s', a')$ is the *TD target*\n",
    "\n",
    "Finally, for efficiency reasons we don't actually have to construct the full Q table as we don't really care about states that we've never experience. So we'll estimate values only for those states that we've seen and iterate over a smaller set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aggregate-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 5000 iterations with best reward: 0.950\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from functools import partial\n",
    "from typing import Callable, NamedTuple, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    state: int\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: int\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Q-Learning agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "    ) -> None:\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def policy(self, s: int) -> int:\n",
    "        \"\"\"Returns the best known action\"\"\"\n",
    "        return np.argmax([self.values[s, a] for a in range(self.n_actions)])\n",
    "\n",
    "    def __iadd__(self, e: Experience) -> \"Agent\":\n",
    "        Q_max = self.values[e.next_state, self.policy(e.next_state)]\n",
    "        td_target = e.reward + self.gamma * Q_max\n",
    "        td_error = td_target - self.values[e.state, e.action]\n",
    "        self.values[e.state, e.action] += self.alpha * td_error\n",
    "        return self\n",
    "\n",
    "\n",
    "def explore(env: gym.Env, state: int) -> Tuple[Experience, int]:\n",
    "    \"\"\"\n",
    "    Samples and applies a random action in given environment from given state.\n",
    "    Returns experience (s, a, r, s') and new state (resets env if necessary).\n",
    "    \"\"\"\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    experience = Experience(state, action, reward, next_state)\n",
    "    state = env.reset() if done else next_state\n",
    "    return experience, state\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    env: gym.Env,\n",
    "    n_episodes: int,\n",
    "    policy: Callable[[int], int],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Runs n episodes in given environment using given policy and\n",
    "    computes the mean non-disounted reward.\n",
    "    \"\"\"\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        episode_done = False\n",
    "\n",
    "        while not episode_done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, episode_done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "    return total_reward / n_episodes\n",
    "\n",
    "\n",
    "def train(\n",
    "    env: gym.Env,\n",
    "    eval_episodes: int = 20,\n",
    "    alpha: float = 0.2,\n",
    "    gamma: float = 0.9,\n",
    "    solution_bound: float = 0.8,\n",
    "    max_iters: int = 5_000,\n",
    ") -> None:\n",
    "    with SummaryWriter(comment=\"-q-learning\") as writer:\n",
    "\n",
    "        # Bind experience sampling and policy evaluation to environments\n",
    "        #  - Note: We use a copy of the env. for testing.\n",
    "        explore_env = partial(explore, env)\n",
    "        eval_policy = partial(evaluate, gym.make(env.spec.id), eval_episodes)\n",
    "\n",
    "        # Initialize the Q-Learning agent\n",
    "        agent = Agent(env.action_space.n, alpha, gamma)\n",
    "\n",
    "        i = 0\n",
    "        reward = 0.0\n",
    "        best_reward = 0.0\n",
    "\n",
    "        # Initialize the environment\n",
    "        state = env.reset()\n",
    "\n",
    "        while reward < solution_bound and i < max_iters:\n",
    "            i += 1\n",
    "\n",
    "            # Sample new experience from the environment\n",
    "            # and pass it to the agent to learn from it.\n",
    "            experience, state = explore_env(state)\n",
    "            agent += experience\n",
    "\n",
    "            # Evaluate current policy\n",
    "            mean_reward = eval_policy(agent.policy)\n",
    "            best_reward = max(mean_reward, best_reward)\n",
    "\n",
    "            # Record metrics\n",
    "            writer.add_scalar(\"reward\", mean_reward, i)\n",
    "            writer.add_scalar(\"best_reward\", best_reward, i)\n",
    "\n",
    "    print(f\"Solved in {i} iterations with best reward: {best_reward:.3f}\")\n",
    "\n",
    "\n",
    "# Run Q-Learning in FrozenLake\n",
    "train(env=gym.make(\"FrozenLake-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-energy",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "Even Q-Learning does not iterate over all possible states, the size of the Q table might become intractable. The idea of *Deep Q-Learning* is to use a deep NN to represent $Q(s, a) \\approx Q(s, a; \\mathbf{w})$ ($\\mathbf{w}$ are the NN parameters). We treat the problem as a supervised regression task and train the network with a SGD on a dataset of collected experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-coating",
   "metadata": {},
   "source": [
    "### Problems in Deep Q-Learning\n",
    "Although a NN model is a compact representation of the Q table, the learning dynamics we described above has some issues:\n",
    "\n",
    "#### Exploration-Exploitation Dilema\n",
    "In all the previous examples we've used random actions to sample experiences from the environment (exploration). However, if we're confident enough in the policy we have, it'd be much better to exploit it and use $a = argmax_a Q(s, a)$ in a state $s$.\n",
    "\n",
    "A straightforward trick that is used to balance exploration and exploitation a bit is called $\\epsilon$*-greedy* policy which randomly switches between these two extremes with $\\epsilon$ probability. We typically define a schedule and decrease $\\epsilon$ as the training proceeds.\n",
    "\n",
    "#### SGD Optimization\n",
    "SGD is heavily based on the assumption of the training instances being *i.i.d* which is definitely not the case in Q-Learning dynamics.\n",
    "\n",
    "First, independence is broken due to the fact further states in an episode do depend on previous actions and states.\n",
    "\n",
    "Furthermore, the training data (experiences) are not identically distributed. Regardless of which policy we use to sample the experiences (during training) it won't produce the same distibution of experiences as if we had used the optimal policy. In short, our targets do not follow the same distribution as what we're trying to learn.\n",
    "\n",
    "To mitigate the i.i.d problem we introduce a *replay buffer* - a large collection of experinces from which we sample training mini-batches.\n",
    "\n",
    "#### Experience Correlations\n",
    "Next issue is that the future Q values $Q(s', a')$ are highly correlated with current $Q(s, a)$ that we try to update. The model starts by modifying $Q(s', a')$ which might cause $Q(s, a)$ to get worse but then by trying fixing $Q(s, a)$ we break $Q(s', a')$. Basically, the training can be quite unstable.\n",
    "\n",
    "A technique that is frequently used to address this is described as *fixing (freezing) the targets*. We introduce second NN $\\hat{Q}$ which is a copy of our $Q$ NN and use DT targets $r + \\gamma \\max_{a'} \\hat{Q}(s', a')$. NN parameters are then regularly synchronized with quite a large period (1k or even 10k iterations).\n",
    "\n",
    "#### The Markov Property\n",
    "Last but not least, in certain environments (e.g. Atari games) the observations do not capture all the information required to make a decesion. For instance, from a single game screenshot we can't tell the dynamics of a ball. This breaks the Markov property we assume in MDPs and shifts the problem to a much harder class of *Partially Observable Markov Decision Processes (POMDPs)*.\n",
    "\n",
    "One common trick to mitigate this issue and pretend we have a MDP is to capture the dynamic by stacking a number of consecutive frames and thread the resulting tensor as single observation. This is sufficient in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-authentication",
   "metadata": {},
   "source": [
    "### DQN Training\n",
    "Putting all the pieces together, here is an overview of the training algorithm for a *Deep Q-Network (DQN)*.\n",
    "1. Initialize parameters of $Q$ and $\\hat{Q}$, start with $\\epsilon \\gets 1$\n",
    "1. Select a random action $a$ with $\\epsilon$ probability, otherwise pick $a = argmax_a Q(s, a)$\n",
    "1. Run $a$ in the environment and gain experience $e = (s, a, r, s')$\n",
    "1. Store $e$ in the replay buffer\n",
    "1. Sample random mini-batch $B$ from the replay buffer\n",
    "1. For each $(s, a, r, s') \\in B$ alculate targets $y$: $y = r$ if episode ends, otherwise $y = r + \\gamma \\max_{a'} \\hat{Q}(s', a')$\n",
    "1. Compute the loss $\\mathcal{L} = (Q(s, a) - y)^2$\n",
    "1. Using $\\mathcal{L}$ run a SGD step to update $Q(s, a)$\n",
    "1. Every $N$ steps copy $\\mathbf{w}_Q \\to \\mathbf{w}_{\\hat{Q}}$\n",
    "1. Repeat from 2. until convergence or other termination condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-occasions",
   "metadata": {},
   "source": [
    "### Atari Environment Wrappers\n",
    "The original Atari DQN paper defines a set of nowadays common environment transformations that make the training easier or even possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opponent-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    For environments where the user need to press FIRE for the game to start.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: Optional[gym.Env] = None) -> None:\n",
    "        super().__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action: int) -> np.ndarray:\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.env.reset()\n",
    "\n",
    "        # Press FIRE\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Return only every `skip`-th frame, the intermediate frames are max-pooled.\n",
    "    This is to prevent the flickering effect in Atari games.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: Optional[gym.Env] = None, skip: int = 4) -> None:\n",
    "        super().__init__(env)\n",
    "        # Small frame buffer to max-poole the last two frames\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action: int) -> np.ndarray:\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        # Make `skip` steps instead of just one and accumulate the reward\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Max pooling of buffered frames\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        # Reset the environment and frame buffer\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.clear()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Reshape, crop and reduce RGB frames to 84x84 grayscale images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: Optional[gym.Env] = None) -> None:\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(84, 84, 1),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "    def observation(self, obs: np.ndarray) -> np.ndarray:\n",
    "        return self.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        # Handle screen shapes in different Atari games\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            shape = [210, 160, 3]\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            shape = [250, 160, 3]\n",
    "        else:\n",
    "            raise Exception(f\"Unknown resolution: {frame.shape}\")\n",
    "\n",
    "        # Reshape the image\n",
    "        img = frame.reshape(shape).astype(np.float32)\n",
    "\n",
    "        # Convert RGB image to grayscale\n",
    "        #  - colorimetric grayscale conversion\n",
    "        #  - img[..., 0] * 0.299 + img[..., 1] * 0.587 + img[..., 2] * 0.114\n",
    "        img = img.dot((0.299, 0.587, 0.114))\n",
    "\n",
    "        # Resize and crop the image to final 84x84x1 tensor\n",
    "        #  - We crop the image to keep the central part\n",
    "        resized_screen = cv2.resize(\n",
    "            src=img,\n",
    "            dsize=(84, 110),\n",
    "            interpolation=cv2.INTER_AREA,\n",
    "        )\n",
    "        cropped_screen = resized_screen[18:102, :]\n",
    "        return np.reshape(cropped_screen, [84, 84, 1]).astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Reshape observations to conform to PyTorch's conventions.\n",
    "\n",
    "    (height, width, channels) -> (channels, height, width)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0,\n",
    "            high=1.0,\n",
    "            shape=new_shape,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
    "        # Move color channel to the front\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"Scale frame intensities to [0, 1] interval\"\"\"\n",
    "\n",
    "    def observation(self, obs: np.ndarray) -> np.ndarray:\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Make a sliding window of N consecutive frames and\n",
    "    return these windows as new observations.\n",
    "    \"\"\"\n",
    "\n",
    "    buffer: np.ndarray\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        n_steps: int,\n",
    "        dtype: np.dtype = np.float32,\n",
    "    ) -> None:\n",
    "        super().__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            old_space.low.repeat(n_steps, axis=0),\n",
    "            old_space.high.repeat(n_steps, axis=0),\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.buffer = np.zeros_like(\n",
    "            self.observation_space.low,\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
    "        # Pop the oldest frame from the buffer and add new one\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "def make_env(env_name: str) -> gym.Env:\n",
    "    \"\"\"Make a new wrapped Atari environment\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-acting",
   "metadata": {},
   "source": [
    "### Atari DQN\n",
    "The DQN architecture is quite simple and consists of two sequential parts:\n",
    "1. Convolutional part with three 2D conv. layers with ReLU activations\n",
    "1. Fully connected part with one dense layer with ReLU activation and final linar layer\n",
    "\n",
    "There is an implicit flatten between these two parts to convert a 4D output of convolutions (batch of 3D images) to 2D input of the FC layer (batch of 1D feature vectors).\n",
    "\n",
    "Finally, the we output Q value for each action as it is more efficient to model $Q(\\cdot, a)$ rather than pass both states and actions on the input and output single $Q(s, a)$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # noqa\n",
    "import torch.nn as nn  # noqa\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape: Tuple[int, ...], n_actions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        n_conv_inputs = input_shape[0]\n",
    "        n_fc_inputs = self._conv_output_dim(input_shape)\n",
    "\n",
    "        # Stack of 2D convolutional layers with ReLU activations\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(n_conv_inputs, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for the regression part\n",
    "        #  - Outputs Q(., a) for each action a\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_fc_inputs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def _conv_output_dim(self, shape: Tuple[int, ...]) -> int:\n",
    "        dummy_conv_input = torch.zeros(1, *shape)\n",
    "        dummy_conv_output = self.conv(dummy_conv_input)\n",
    "        return int(np.prod(dummy_conv_output.size()))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # `view(batch_size, -1)` flattens all the feature dimensions\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-collar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
